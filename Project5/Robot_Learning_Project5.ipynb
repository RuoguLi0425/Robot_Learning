{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RuoguLi0425/Robot-Learning/blob/main/Robot_Learning_Project5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tp8PVN5sJ9pd"
      },
      "source": [
        "# ***Important***\n",
        "\n",
        "**Before starting, make sure to read the [Assignment Instructions](https://courseworks2.columbia.edu/courses/172081/pages/assignment-instructions) page on Courseworks2 to learn the workflow for completing this project.**\n",
        "\n",
        "- *Apart from the link to your notebook, you are also required to submit `q_network.pth` of Part 1 and `ppo_network.zip` (model checkpoints are loaded and saved by stable_baselines3 as zip files) of Part 2 to Coursework. You should put the link to your notebook in the comment entry.*\n",
        "- *Please name the revision you want us to grade \"Grade me\". We will only grade the revision that is correctly named. Late days will be applied according to this named revision.*\n",
        "- *Please make sure that \"anyone in LionMail with the link\" has Edit permissions.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inY7y5CRo97q"
      },
      "source": [
        "# Project Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPIiNSZ8hb8Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6014e4f2-f0b5-482d-ba12-1b0493dd5ef0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mecs6616_sp23_project5'...\n",
            "remote: Enumerating objects: 43, done.\u001b[K\n",
            "remote: Counting objects: 100% (43/43), done.\u001b[K\n",
            "remote: Compressing objects: 100% (36/36), done.\u001b[K\n",
            "remote: Total 43 (delta 19), reused 23 (delta 7), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (43/43), 13.83 KiB | 885.00 KiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "# DO NOT CHANGE\n",
        "\n",
        "# After running this cell, the folder 'mecs6616_sp23_project3' will show up in the file explorer on the left (click on the folder icon if it's not open)\n",
        "# It may take a few seconds to appear\n",
        "!git clone https://github.com/roamlab/mecs6616_sp23_project5.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ise8RAQhhs3X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4406c6f0-a063-454a-9872-1d5ab5598ee3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'/content/mecs6616_sp23_project5/arm_dynamics_base.py' -> '/content/arm_dynamics_base.py'\n",
            "'/content/mecs6616_sp23_project5/arm_dynamics.py' -> '/content/arm_dynamics.py'\n",
            "'/content/mecs6616_sp23_project5/arm_env.py' -> '/content/arm_env.py'\n",
            "'/content/mecs6616_sp23_project5/geometry.py' -> '/content/geometry.py'\n",
            "'/content/mecs6616_sp23_project5/render.py' -> '/content/render.py'\n",
            "'/content/mecs6616_sp23_project5/robot.py' -> '/content/robot.py'\n",
            "'/content/mecs6616_sp23_project5/score.py' -> '/content/score.py'\n"
          ]
        }
      ],
      "source": [
        "# DO NOT CHANGE\n",
        "\n",
        "# copy all needed files into the working directory. This is simply to make accessing files easier\n",
        "!cp -av /content/mecs6616_sp23_project5/* /content/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT CHANGE\n",
        "\n",
        "# There will be error messages from this command. You can ignore those error messages\n",
        "# as long as you see \"Successfully installed setuptools-65.5.0\" at the end.\n",
        "\n",
        "!pip install setuptools==65.5.0"
      ],
      "metadata": {
        "id": "gYlcOXqbosEA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85bb0077-6ff0-4648-f54d-6a19c5a98e36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: setuptools==65.5.0 in /usr/local/lib/python3.10/dist-packages (65.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT CHANGE\n",
        "\n",
        "# There will be error messages from this command. You can ignore those error messages\n",
        "# as long as you see \"Successfully installed gym-0.21.0 stable-baselines3-1.5.0\" at the end.\n",
        "\n",
        "!pip install gym==0.21.0 stable-baselines3==1.5.0"
      ],
      "metadata": {
        "id": "RChyEFYYqLGl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3483b50-f0e2-4a15-b2c2-90dd9446a62d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gym==0.21.0\n",
            "  Downloading gym-0.21.0.tar.gz (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting stable-baselines3==1.5.0\n",
            "  Downloading stable_baselines3-1.5.0-py3-none-any.whl (177 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.7/177.7 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym==0.21.0) (1.22.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym==0.21.0) (2.2.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3==1.5.0) (3.7.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3==1.5.0) (1.5.3)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3==1.5.0) (2.0.0+cu118)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->stable-baselines3==1.5.0) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->stable-baselines3==1.5.0) (2.0.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->stable-baselines3==1.5.0) (1.11.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->stable-baselines3==1.5.0) (4.5.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->stable-baselines3==1.5.0) (3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->stable-baselines3==1.5.0) (3.12.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->stable-baselines3==1.5.0) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->stable-baselines3==1.5.0) (16.0.2)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==1.5.0) (4.39.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==1.5.0) (0.11.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==1.5.0) (3.0.9)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==1.5.0) (1.0.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==1.5.0) (23.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==1.5.0) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==1.5.0) (8.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==1.5.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3==1.5.0) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3==1.5.0) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->stable-baselines3==1.5.0) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->stable-baselines3==1.5.0) (1.3.0)\n",
            "Building wheels for collected packages: gym\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for gym\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for gym\n",
            "Failed to build gym\n",
            "Installing collected packages: gym, stable-baselines3\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "  Running setup.py install for gym ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  DEPRECATION: gym was installed using the legacy 'setup.py install' method, because a wheel could not be built for it. pip 23.1 will enforce this behaviour change. A possible replacement is to fix the wheel build issue reported above. Discussion can be found at https://github.com/pypa/pip/issues/8368\u001b[0m\u001b[33m\n",
            "\u001b[0mSuccessfully installed gym-0.21.0 stable-baselines3-1.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Implement DQN\n",
        "\n",
        "For this part, you will implement DQN from scratch. You SHOULD NOT use any RL libraries."
      ],
      "metadata": {
        "id": "-KNg9fzU5Un7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-JvzRuwNsYz"
      },
      "source": [
        "## Starter Code Explanation\n",
        "In addition to code you are already familiar with from the previous project (i.e. arm dynamics, etc.) we are providing an \"Environment\" in the `ArmEnv` class. The environment \"wraps around\" the arm dynamics and provides the key functions that an RL algorithm expects: reset(...) and step(...). The implementation of `ArmEnv` follows the [OpenAI Gym](https://www.gymlibrary.dev/api/core/) API standard. It is a standard that is accepeted by many RL libraries and allows for our problem to be easily solved with various RL libraries. Take a moment to familiarize yourself with these functions! See [here](https://www.gymlibrary.dev/api/core/) for more information on the definition of the reset(...) and step(...) functions.\n",
        "\n",
        "Important notes:\n",
        "\n",
        "* The ArmEnv expects an action similar to the one used previously: a vector with a torque for every arm joint. Thus, the native action space for this environment is high-dimensional, and continuous. DQN will require an action space that is 1-dimensional and discrete. You will need to convert between these. For example, you can have an action space of [0, 1, 2,] where each number just represents the identity of an action candidate, and a conversion dictionary {0: [-0.1, -0.1], 1: [0.1, 0.1], 2: [0, 0]}. Then, when the Q network output an action 1, it will be converted into [0.1, 0.1] and used by the environment. Note that this is just an example method to implement the conversion and you do not have to follow the same procedure.\n",
        "* The observation provided by the environment will comprise the same state vector as before, to which we append the current position of the end_effector and the goal for the end-effector. Since your policy must learn to reach arbitrary goals, the goal must be provided as part of the observation. So the observation will consist of 8 values: 4 for the state, 2 for the pos_ee and 2 for the goal.\n",
        "* The maximum episode length of the environment is 200 steps. Each step is simulated for 0.01 second. This should be used for both training and testing.\n",
        "* The reward function of this environment is by default r(s, a) = - dist(pos_ee, goal)^2 where represents the negative square of L2 distance between the current position of the end-effector and the goal position."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Arm Environment Example\n",
        "You are encouraged to view the `arm_env.py` file to understand the `random_goal()`, `reset()` and `step()`  functions but do not modify the file.\n",
        "\n",
        "The `env.reset()` method, will reset the arm in the vertically downwards position and set a new random goal by calling the `random_goal()` method. By understanding how the goals are set you could guide your training in that direction. You can also provide your own goal as a (2,1) array to the reset function as an argument. This could come handy later when training the model.\n",
        "\n",
        "The `env.step()` function takes an action as a (2,1) shaped array and outputs the next observation, reward, done and info. `info` is a dictionary with pos_ee and vel_ee values. This can come handy if you attempt to do some reward engineering.\n",
        "\n",
        "The cell below provides an example of random policy interacting with the ArmEnv for 50 steps (0.5 seconds)"
      ],
      "metadata": {
        "id": "gw8H0PZcSv7F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 830
        },
        "outputId": "80414257-92b6-41e9-cc54-388c1222f214",
        "id": "o6r9kJ5jpeds"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1000 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "from render import Renderer\n",
        "from arm_env import ArmEnv\n",
        "from robot import Robot\n",
        "from arm_dynamics import ArmDynamics\n",
        "import numpy as np\n",
        "\n",
        "# DO NOT CHANGE arm parameters\n",
        "arm = Robot(\n",
        "        ArmDynamics(\n",
        "            num_links=2,\n",
        "            link_mass=0.1,\n",
        "            link_length=1,\n",
        "            joint_viscous_friction=0.1,\n",
        "            dt=0.01,\n",
        "\t    \t\t\tgravity=False\n",
        "        )\n",
        "    )\n",
        "arm.reset()\n",
        "# ------------------\n",
        "\n",
        "env = ArmEnv(arm, gui=True)\n",
        "\n",
        "# Passing our own defined goal to the reset function\n",
        "# goal = np.array([[0.5], [-1.5]])\n",
        "# obs = env.reset(goal)\n",
        "\n",
        "# Resetting the environment without the goal will set a random goal position\n",
        "obs = env.reset()\n",
        "\n",
        "for _ in range(50):\n",
        "  rand_action = np.random.uniform(-1.5, 1.5, (2,1))\n",
        "  obs, reward, done, info = env.step(rand_action)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jmXTT_ngdqG"
      },
      "source": [
        "### QNetwork\n",
        "This class defines the architecture of your network. You must fill in the __init__(...) function which defines your network, and the forward(...) function which performs the forward pass.\n",
        "\n",
        "Your action space should be discrete, with whatever cardinality you decide. The size of the output layer of your Q-Network should thus be the same as the cardinality of your action space. When selecting an action, a policy must choose the one that has the highest estimated Q-value for the current state. As part of the QNetwork class, we are providing the function select_discrete_action(...) which does exactly that.\n",
        "\n",
        "The arm environment itself however expects a 2-dimensional, continuous action vector. Therefore, when it comes to send an action to the environment, you must provide the kind of action the environment expects. It is your job to determine how to convert between the discrete action space of your Q-Network and the continuous action space of the arm. You do this by filling in the action_discrete_to_continuous(...) function in your QNetwork. You can expect to call the step function of the environment like this:\n",
        "\n",
        "```\n",
        "self.env.step(self.q_network.action_discrete_to_continuous(discrete_action))\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7UyguLRKgf_I"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "class QNetwork(nn.Module):\n",
        "  def __init__(self, env):\n",
        "    super(QNetwork, self).__init__()\n",
        "    #--------- YOUR CODE HERE --------------\n",
        "    self.fc1 = nn.Linear(env.observation_space.shape[0], 128)\n",
        "    self.fc2 = nn.Linear(128, 64)\n",
        "    self.fc3 = nn.Linear(64, 5)\n",
        "    #---------------------------------------\n",
        "\n",
        "  def forward(self, x, device):\n",
        "    #--------- YOUR CODE HERE --------------\n",
        "    x = torch.tensor(x, dtype=torch.float32, device=device)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return x\n",
        "    #---------------------------------------\n",
        "\n",
        "\n",
        "  def select_discrete_action(self, obs, device):\n",
        "    # Put the observation through the network to estimate q values for all possible discrete actions\n",
        "    est_q_vals = self.forward(obs.reshape((1,) + obs.shape), device)\n",
        "    # Choose the discrete action with the highest estimated q value\n",
        "    discrete_action = torch.argmax(est_q_vals, dim=1).tolist()[0]\n",
        "    return discrete_action\n",
        "\n",
        "  def action_discrete_to_continuous(self, discrete_action):\n",
        "    #--------- YOUR CODE HERE --------------\n",
        "    action_map = {\n",
        "        0: np.array([-1.0, -1.0]),\n",
        "        1: np.array([-1.0, 1.0]),\n",
        "        2: np.array([1.0, -1.0]),\n",
        "        3: np.array([1.0, 1.0]),\n",
        "        4: np.array([0.0, 0.0]),\n",
        "\n",
        "    }\n",
        "\n",
        "    continuous_action = action_map[discrete_action]\n",
        "    return continuous_action.reshape(2, -1)\n",
        "    #---------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We provide you with code to use the replay buffer in your RL implementation. You do not need to change the ReplayBuffer class.\n",
        "```\n",
        "rb = ReplayBuffer()\n",
        "```\n",
        "After creating a ReplayBuffer object you can add samples in the buffer using `put()`:\n",
        "```\n",
        "rb.put((obs, action, reward, next_obs, done))\n",
        "```\n",
        "Take random samples from the buffer using:\n",
        "```\n",
        "obs, actions, rewards, next_obses, dones = rb.sample(batch_size)\n",
        "```\n"
      ],
      "metadata": {
        "id": "IUjAeQcPdsGR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7NytRAXtYkE"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class ReplayBuffer():\n",
        "    def __init__(self, buffer_limit):\n",
        "        self.buffer = collections.deque(maxlen=buffer_limit)\n",
        "\n",
        "    def put(self, transition):\n",
        "        self.buffer.append(transition)\n",
        "\n",
        "    def sample(self, n):\n",
        "        mini_batch = random.sample(self.buffer, n)\n",
        "        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n",
        "\n",
        "        for transition in mini_batch:\n",
        "            s, a, r, s_prime, done_mask = transition\n",
        "            s_lst.append(s)\n",
        "            a_lst.append(a)\n",
        "            r_lst.append(r)\n",
        "            s_prime_lst.append(s_prime)\n",
        "            done_mask_lst.append(done_mask)\n",
        "\n",
        "        return np.array(s_lst), np.array(a_lst), \\\n",
        "               np.array(r_lst), np.array(s_prime_lst), \\\n",
        "               np.array(done_mask_lst)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TrainDQN\n",
        "Here, you must fill in the train(...) function that actually trains your network.\n",
        "\n",
        "We are providing a helper function called save_model(...) that will save the current Q-network. Use this as you see fit.\n",
        "\n",
        "To set one network equal to another one, you can use code like this:\n",
        "```\n",
        "target_network.load_state_dict(self.q_network.state_dict())\n",
        "```\n",
        "\n",
        "If you would like to be graded with a specific seed for the random number generators, make sure to change the default seed in the initialization of the TrainDQN class.\n",
        "\n",
        "The time taken to train the model will depend mainly on how big is your model architecture and the number of episodes you run the training for. As a reference, the time taken to train a model on 1500 episodes, which passed all evaluation metrics was about an hour.\n",
        "* Reference value for clipping the gradient value as mentioned in class: 0.2\n",
        "* Reference value for a typical size of Replay Buffer: >10k\n",
        "* Reference value for batch size while training: 64 - 512\n",
        "\n",
        "Note that these are just reference values and larger is not always better as it may slow things down.\n",
        "\n",
        "It is good practice in RL to ensure simpler things are working before complicating environments or training techniques.\n",
        "\n",
        "If you think your training method is not working at all, you could pass a fixed goal to the `env.reset()` method during the training loop to ensure that your model is learning."
      ],
      "metadata": {
        "id": "pxVawoBLe3bd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwS8xVR7tbeQ"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from render import Renderer\n",
        "from arm_env import ArmEnv\n",
        "import numpy as np\n",
        "from math import dist\n",
        "\n",
        "\n",
        "class TrainDQN:\n",
        "\n",
        "  def __init__(self, env, seed=0):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    self.env = env\n",
        "    self.device = torch.device('cpu')\n",
        "    self.q_network = QNetwork(env).to(self.device)\n",
        "    self.target_network = QNetwork(env).to(self.device)\n",
        "    self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "\n",
        "  def save_model(self, episode_num, save_dir='models'):\n",
        "    timestr = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "    model_dir = os.path.join(save_dir, timestr)\n",
        "    if not os.path.exists(os.path.join(model_dir)):\n",
        "      os.makedirs(os.path.join(model_dir))\n",
        "    savepath = os.path.join(model_dir, f'q_network_ep_{episode_num:04d}.pth')\n",
        "    torch.save(self.q_network.state_dict(), savepath)\n",
        "    print(f'model saved to {savepath}\\n')\n",
        "\n",
        "\n",
        "  def train(self):\n",
        "\n",
        "    #--------- YOUR CODE HERE --------------\n",
        "\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "    import torch.optim as optim\n",
        "\n",
        "    capacity = 10000\n",
        "    replay_buffer = ReplayBuffer(buffer_limit=capacity)\n",
        "    batch_size = 64\n",
        "    gamma = 0.99\n",
        "    learning_rate = 0.0001\n",
        "    optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
        "    criterion = nn.MSELoss()\n",
        "    epsilon = 1.0\n",
        "    epsilon_decay = 0.995\n",
        "    epsilon_min = 0.01\n",
        "    num_episodes = 1500\n",
        "    update_target_network_freq = 25\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        episode_reward = 0\n",
        "        state = self.env.reset()\n",
        "\n",
        "        while True:\n",
        "            if random.random() < epsilon:\n",
        "                discrete_action = np.random.randint(0, 5)\n",
        "            else:\n",
        "                discrete_action = self.q_network.select_discrete_action(state, self.device)\n",
        "\n",
        "            continuous_action = self.q_network.action_discrete_to_continuous(discrete_action)\n",
        "            next_state, reward, done, _ = self.env.step(continuous_action)\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "            replay_buffer.put((state, discrete_action, reward, next_state, done))\n",
        "            episode_reward += reward\n",
        "\n",
        "            if len(replay_buffer.buffer) >= batch_size:\n",
        "                s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = replay_buffer.sample(batch_size)\n",
        "                s_lst = torch.from_numpy(s_lst).float()\n",
        "                a_lst = torch.from_numpy(a_lst).long()\n",
        "                r_lst = torch.from_numpy(r_lst).float()\n",
        "                s_prime_lst = torch.from_numpy(s_prime_lst).float()\n",
        "\n",
        "                q = self.q_network.forward(s_lst, self.device).gather(1, a_lst.view(-1, 1)).squeeze()\n",
        "                q_t = self.target_network.forward(s_prime_lst, self.device)\n",
        "                q_t_max = torch.max(q_t.detach(), dim=1)[0]\n",
        "                q_tar = r_lst + gamma * q_t_max\n",
        "\n",
        "                loss = criterion(q, q_tar)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "\n",
        "                for p in self.q_network.parameters():\n",
        "                    p.grad.data.clamp_(-5, 5)\n",
        "\n",
        "                optimizer.step()\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "        if episode % update_target_network_freq == 0:\n",
        "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "\n",
        "        # Update epsilon\n",
        "        epsilon = max(epsilon_min, epsilon_decay * epsilon)\n",
        "\n",
        "        print(\"Episode:\", episode, \"  reward:\", episode_reward)\n",
        "        # Save model\n",
        "\n",
        "        if episode % 500 == 0:\n",
        "          self.save_model(episode)\n",
        "\n",
        "    pass\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEHSV1Q1BT1t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ded12f0-11b8-4114-98d7-59f59f2a6d71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-c8113dbe9860>:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x, dtype=torch.float32, device=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 0   reward: -446.3019522580346\n",
            "model saved to models/2023-05-03_03-27-08/q_network_ep_0000.pth\n",
            "\n",
            "Episode: 1   reward: -1254.008664724819\n",
            "Episode: 2   reward: -580.255641324242\n",
            "Episode: 3   reward: -1071.6239544385935\n",
            "Episode: 4   reward: -1135.8035200519944\n",
            "Episode: 5   reward: -130.90559923589447\n",
            "Episode: 6   reward: -499.35298650028903\n",
            "Episode: 7   reward: -1411.4744120685266\n",
            "Episode: 8   reward: -791.0382112441325\n",
            "Episode: 9   reward: -581.6881160257286\n",
            "Episode: 10   reward: -262.24052877579715\n",
            "Episode: 11   reward: -323.56747440948953\n",
            "Episode: 12   reward: -707.6383207666443\n",
            "Episode: 13   reward: -1349.768181242778\n",
            "Episode: 14   reward: -1393.2293445634414\n",
            "Episode: 15   reward: -1070.633480897608\n",
            "Episode: 16   reward: -474.1408512864749\n",
            "Episode: 17   reward: -326.1851965246223\n",
            "Episode: 18   reward: -840.8779605977538\n",
            "Episode: 19   reward: -1003.7300303495078\n",
            "Episode: 20   reward: -635.1320068036862\n",
            "Episode: 21   reward: -181.1968905796687\n",
            "Episode: 22   reward: -391.5285827716801\n",
            "Episode: 23   reward: -1322.393227350891\n",
            "Episode: 24   reward: -516.0680916816007\n",
            "Episode: 25   reward: -878.9437802512897\n",
            "Episode: 26   reward: -793.4435353005383\n",
            "Episode: 27   reward: -1281.9378856189287\n",
            "Episode: 28   reward: -480.97450762925257\n",
            "Episode: 29   reward: -468.5016579526039\n",
            "Episode: 30   reward: -228.42774726831942\n",
            "Episode: 31   reward: -195.65722259242736\n",
            "Episode: 32   reward: -910.0581303495785\n",
            "Episode: 33   reward: -1522.6091574878994\n",
            "Episode: 34   reward: -1147.1224444136985\n",
            "Episode: 35   reward: -678.8040549567751\n",
            "Episode: 36   reward: -605.3510171062204\n",
            "Episode: 37   reward: -701.6526469099128\n",
            "Episode: 38   reward: -629.5123462600776\n",
            "Episode: 39   reward: -161.4685053465741\n",
            "Episode: 40   reward: -607.6851015539444\n",
            "Episode: 41   reward: -1378.9831758716664\n",
            "Episode: 42   reward: -969.5714498538727\n",
            "Episode: 43   reward: -604.9878590556591\n",
            "Episode: 44   reward: -681.0793036715169\n",
            "Episode: 45   reward: -350.6153147284731\n",
            "Episode: 46   reward: -208.63533348731028\n",
            "Episode: 47   reward: -861.3385412300194\n",
            "Episode: 48   reward: -515.5202739769514\n",
            "Episode: 49   reward: -443.9263789811825\n",
            "Episode: 50   reward: -1070.4933124163274\n",
            "Episode: 51   reward: -895.2020325883714\n",
            "Episode: 52   reward: -325.3888862489236\n",
            "Episode: 53   reward: -631.246024208455\n",
            "Episode: 54   reward: -104.73711214544377\n",
            "Episode: 55   reward: -651.8531262604013\n",
            "Episode: 56   reward: -346.27293295278946\n",
            "Episode: 57   reward: -1105.7491827406448\n",
            "Episode: 58   reward: -282.3685273110515\n",
            "Episode: 59   reward: -346.73158061684455\n",
            "Episode: 60   reward: -410.3207078530242\n",
            "Episode: 61   reward: -258.3108604358988\n",
            "Episode: 62   reward: -522.9582703095207\n",
            "Episode: 63   reward: -706.9527883449975\n",
            "Episode: 64   reward: -424.3923455594336\n",
            "Episode: 65   reward: -451.2230636420493\n",
            "Episode: 66   reward: -591.6557909128345\n",
            "Episode: 67   reward: -851.4565277561002\n",
            "Episode: 68   reward: -521.6852151037353\n",
            "Episode: 69   reward: -286.96668555451424\n",
            "Episode: 70   reward: -347.90511963144445\n",
            "Episode: 71   reward: -883.1398445921302\n",
            "Episode: 72   reward: -713.852603354473\n",
            "Episode: 73   reward: -464.7933231270851\n",
            "Episode: 74   reward: -394.335959477285\n",
            "Episode: 75   reward: -709.2344971677335\n",
            "Episode: 76   reward: -446.49155979251213\n",
            "Episode: 77   reward: -1165.111955401116\n",
            "Episode: 78   reward: -295.2108566645427\n",
            "Episode: 79   reward: -403.7296983267108\n",
            "Episode: 80   reward: -367.3037504153028\n",
            "Episode: 81   reward: -482.5853882214532\n",
            "Episode: 82   reward: -397.9936141047607\n",
            "Episode: 83   reward: -720.0446034674769\n",
            "Episode: 84   reward: -748.1557546712293\n",
            "Episode: 85   reward: -484.0508161524412\n",
            "Episode: 86   reward: -647.3731707196794\n",
            "Episode: 87   reward: -386.1532758687476\n",
            "Episode: 88   reward: -75.76604649720682\n",
            "Episode: 89   reward: -221.96295691978136\n",
            "Episode: 90   reward: -496.80088442533145\n",
            "Episode: 91   reward: -412.4384322693863\n",
            "Episode: 92   reward: -359.4573598649435\n",
            "Episode: 93   reward: -815.4838299058368\n",
            "Episode: 94   reward: -342.92056217977546\n",
            "Episode: 95   reward: -287.61001828460815\n",
            "Episode: 96   reward: -246.9192037125416\n",
            "Episode: 97   reward: -377.95588815933564\n",
            "Episode: 98   reward: -378.113761889068\n",
            "Episode: 99   reward: -464.8477117865792\n",
            "Episode: 100   reward: -284.7178164332061\n",
            "Episode: 101   reward: -312.23423662527745\n",
            "Episode: 102   reward: -419.6489424744526\n",
            "Episode: 103   reward: -378.1147378960072\n",
            "Episode: 104   reward: -171.78345351017867\n",
            "Episode: 105   reward: -355.34344185910857\n",
            "Episode: 106   reward: -421.95265736674736\n",
            "Episode: 107   reward: -109.67676872392096\n",
            "Episode: 108   reward: -251.54261926690225\n",
            "Episode: 109   reward: -264.8096124721163\n",
            "Episode: 110   reward: -339.882026537217\n",
            "Episode: 111   reward: -391.79968859808577\n",
            "Episode: 112   reward: -464.0813506951836\n",
            "Episode: 113   reward: -267.80904986778484\n",
            "Episode: 114   reward: -665.8511749639296\n",
            "Episode: 115   reward: -390.20397048961\n",
            "Episode: 116   reward: -182.72629291888947\n",
            "Episode: 117   reward: -301.76142072903554\n",
            "Episode: 118   reward: -909.2012078225574\n",
            "Episode: 119   reward: -144.8245402590161\n",
            "Episode: 120   reward: -299.61338023037416\n",
            "Episode: 121   reward: -178.01872232470785\n",
            "Episode: 122   reward: -673.7613248407624\n",
            "Episode: 123   reward: -147.12731118158183\n",
            "Episode: 124   reward: -319.82429630187136\n",
            "Episode: 125   reward: -284.18452613192386\n",
            "Episode: 126   reward: -1068.4344455793216\n",
            "Episode: 127   reward: -208.48556582029542\n",
            "Episode: 128   reward: -609.4982374565856\n",
            "Episode: 129   reward: -164.79679772775492\n",
            "Episode: 130   reward: -482.02804265467665\n",
            "Episode: 131   reward: -160.79921706741854\n",
            "Episode: 132   reward: -201.9801500647402\n",
            "Episode: 133   reward: -503.08196987612826\n",
            "Episode: 134   reward: -464.767939364389\n",
            "Episode: 135   reward: -342.36989029430373\n",
            "Episode: 136   reward: -125.19159532179906\n",
            "Episode: 137   reward: -368.41508179523026\n",
            "Episode: 138   reward: -716.0855509167161\n",
            "Episode: 139   reward: -452.21694039320147\n",
            "Episode: 140   reward: -822.6324928274507\n",
            "Episode: 141   reward: -552.0243755517811\n",
            "Episode: 142   reward: -317.11973852389\n",
            "Episode: 143   reward: -728.6581405468517\n",
            "Episode: 144   reward: -732.7910855611983\n",
            "Episode: 145   reward: -512.0888039764908\n",
            "Episode: 146   reward: -407.7242799830638\n",
            "Episode: 147   reward: -252.7287345590016\n",
            "Episode: 148   reward: -743.6637116301389\n",
            "Episode: 149   reward: -195.358221281895\n",
            "Episode: 150   reward: -216.17432350451577\n",
            "Episode: 151   reward: -214.95642505078803\n",
            "Episode: 152   reward: -124.90075560378517\n",
            "Episode: 153   reward: -679.2138228134819\n",
            "Episode: 154   reward: -306.3853048047979\n",
            "Episode: 155   reward: -554.4834395718322\n",
            "Episode: 156   reward: -959.7947101781963\n",
            "Episode: 157   reward: -504.26130043576467\n",
            "Episode: 158   reward: -991.5540352151406\n",
            "Episode: 159   reward: -585.524374296031\n",
            "Episode: 160   reward: -389.40518650119236\n",
            "Episode: 161   reward: -1024.532109714036\n",
            "Episode: 162   reward: -218.84891696691508\n",
            "Episode: 163   reward: -501.2522330910412\n",
            "Episode: 164   reward: -68.41682880491365\n",
            "Episode: 165   reward: -65.46944216732369\n",
            "Episode: 166   reward: -390.93939366792637\n",
            "Episode: 167   reward: -140.06991721497923\n",
            "Episode: 168   reward: -688.2216744954488\n",
            "Episode: 169   reward: -330.3951995549456\n",
            "Episode: 170   reward: -483.6248732141535\n",
            "Episode: 171   reward: -357.61497978684133\n",
            "Episode: 172   reward: -306.34823182692844\n",
            "Episode: 173   reward: -586.2010381675667\n",
            "Episode: 174   reward: -451.56829474103756\n",
            "Episode: 175   reward: -252.27209845654102\n",
            "Episode: 176   reward: -330.9472782932461\n",
            "Episode: 177   reward: -761.8455033055217\n",
            "Episode: 178   reward: -284.58460999964603\n",
            "Episode: 179   reward: -369.4995584660872\n",
            "Episode: 180   reward: -469.9682305172873\n",
            "Episode: 181   reward: -283.6794607983436\n",
            "Episode: 182   reward: -395.8457910671597\n",
            "Episode: 183   reward: -324.1132424610694\n",
            "Episode: 184   reward: -408.9669480217837\n",
            "Episode: 185   reward: -436.36813144738437\n",
            "Episode: 186   reward: -519.2469779137839\n",
            "Episode: 187   reward: -461.3580254769621\n",
            "Episode: 188   reward: -75.63861416901878\n",
            "Episode: 189   reward: -707.3449319403821\n",
            "Episode: 190   reward: -358.43904512869005\n",
            "Episode: 191   reward: -448.9550518469063\n",
            "Episode: 192   reward: -408.23067085560433\n",
            "Episode: 193   reward: -335.2615226179828\n",
            "Episode: 194   reward: -175.81391690535236\n",
            "Episode: 195   reward: -201.45426733523252\n",
            "Episode: 196   reward: -57.527288957902584\n",
            "Episode: 197   reward: -80.21411125346646\n",
            "Episode: 198   reward: -378.1148737859092\n",
            "Episode: 199   reward: -383.4404550443988\n",
            "Episode: 200   reward: -196.52780934809815\n",
            "Episode: 201   reward: -227.64860129186897\n",
            "Episode: 202   reward: -653.7390165587589\n",
            "Episode: 203   reward: -264.1151559246408\n",
            "Episode: 204   reward: -40.46917730845329\n",
            "Episode: 205   reward: -185.66052385777462\n",
            "Episode: 206   reward: -167.9087129479317\n",
            "Episode: 207   reward: -186.7763231509452\n",
            "Episode: 208   reward: -34.43103257739763\n",
            "Episode: 209   reward: -31.944442881204186\n",
            "Episode: 210   reward: -279.7962469101179\n",
            "Episode: 211   reward: -318.4542480102003\n",
            "Episode: 212   reward: -28.97406896822668\n",
            "Episode: 213   reward: -310.86232055491655\n",
            "Episode: 214   reward: -33.98893912112977\n",
            "Episode: 215   reward: -266.4583762446976\n",
            "Episode: 216   reward: -186.06070831522024\n",
            "Episode: 217   reward: -296.2001135744235\n",
            "Episode: 218   reward: -73.00183322979828\n",
            "Episode: 219   reward: -195.24763061635366\n",
            "Episode: 220   reward: -246.20255642999751\n",
            "Episode: 221   reward: -452.4979294672829\n",
            "Episode: 222   reward: -106.08152110333236\n",
            "Episode: 223   reward: -44.48765645267537\n",
            "Episode: 224   reward: -220.22082140769848\n",
            "Episode: 225   reward: -473.48588214020066\n",
            "Episode: 226   reward: -284.07518333017475\n",
            "Episode: 227   reward: -130.40237412779592\n",
            "Episode: 228   reward: -157.4499426749286\n",
            "Episode: 229   reward: -26.50037198830824\n",
            "Episode: 230   reward: -415.6203444482094\n",
            "Episode: 231   reward: -8.403411506512827\n",
            "Episode: 232   reward: -67.09309508951218\n",
            "Episode: 233   reward: -163.45430640746937\n",
            "Episode: 234   reward: -76.81859971707182\n",
            "Episode: 235   reward: -41.63184089167867\n",
            "Episode: 236   reward: -485.38171725404146\n",
            "Episode: 237   reward: -171.71398859915377\n",
            "Episode: 238   reward: -127.31317471446701\n",
            "Episode: 239   reward: -99.17725830113594\n",
            "Episode: 240   reward: -113.65443416998286\n",
            "Episode: 241   reward: -39.62473106603131\n",
            "Episode: 242   reward: -166.63584079882588\n",
            "Episode: 243   reward: -51.21632170875373\n",
            "Episode: 244   reward: -190.0445978899885\n",
            "Episode: 245   reward: -39.82849231705944\n",
            "Episode: 246   reward: -44.30099906871346\n",
            "Episode: 247   reward: -109.10200716260964\n",
            "Episode: 248   reward: -100.69616190645972\n",
            "Episode: 249   reward: -163.04371833177711\n",
            "Episode: 250   reward: -124.31391618290996\n",
            "Episode: 251   reward: -49.482647844323\n",
            "Episode: 252   reward: -55.919198955245136\n",
            "Episode: 253   reward: -80.85467090385113\n",
            "Episode: 254   reward: -80.14907788047331\n",
            "Episode: 255   reward: -174.0471885015821\n",
            "Episode: 256   reward: -44.30913828341907\n",
            "Episode: 257   reward: -142.52429249455366\n",
            "Episode: 258   reward: -23.526944040885247\n",
            "Episode: 259   reward: -19.058797261347696\n",
            "Episode: 260   reward: -14.13715010702682\n",
            "Episode: 261   reward: -20.06905128176452\n",
            "Episode: 262   reward: -114.80392850223132\n",
            "Episode: 263   reward: -22.20726733061607\n",
            "Episode: 264   reward: -71.43048326451452\n",
            "Episode: 265   reward: -81.67590294951589\n",
            "Episode: 266   reward: -128.27159287282842\n",
            "Episode: 267   reward: -300.7091510905363\n",
            "Episode: 268   reward: -45.75944798961569\n",
            "Episode: 269   reward: -46.304948354475265\n",
            "Episode: 270   reward: -61.73474787386022\n",
            "Episode: 271   reward: -61.53996716435256\n",
            "Episode: 272   reward: -17.515398909196698\n",
            "Episode: 273   reward: -88.892295372315\n",
            "Episode: 274   reward: -309.88801292521936\n",
            "Episode: 275   reward: -49.43004965705588\n",
            "Episode: 276   reward: -95.20720096007771\n",
            "Episode: 277   reward: -105.71589951547928\n",
            "Episode: 278   reward: -19.086742999878556\n",
            "Episode: 279   reward: -32.47446448157674\n",
            "Episode: 280   reward: -77.58354052182604\n",
            "Episode: 281   reward: -13.654987302942295\n",
            "Episode: 282   reward: -26.1599119358085\n",
            "Episode: 283   reward: -18.674084281277363\n",
            "Episode: 284   reward: -8.248618449518178\n",
            "Episode: 285   reward: -47.71277790566118\n",
            "Episode: 286   reward: -32.487620884124325\n",
            "Episode: 287   reward: -24.624575463144033\n",
            "Episode: 288   reward: -77.11149037559352\n",
            "Episode: 289   reward: -79.8036180306453\n",
            "Episode: 290   reward: -110.0608012117088\n",
            "Episode: 291   reward: -30.062288802615893\n",
            "Episode: 292   reward: -95.40816296588582\n",
            "Episode: 293   reward: -24.953822519224026\n",
            "Episode: 294   reward: -22.948652782092495\n",
            "Episode: 295   reward: -25.433470511744382\n",
            "Episode: 296   reward: -25.856496824822\n",
            "Episode: 297   reward: -17.573123333221645\n",
            "Episode: 298   reward: -24.859050182159585\n",
            "Episode: 299   reward: -22.962592640864706\n",
            "Episode: 300   reward: -34.71026236052793\n",
            "Episode: 301   reward: -164.5787280596893\n",
            "Episode: 302   reward: -3.08454099393027\n",
            "Episode: 303   reward: -31.681986096669863\n",
            "Episode: 304   reward: -11.09046641323442\n",
            "Episode: 305   reward: -17.174581527824184\n",
            "Episode: 306   reward: -30.141770918776345\n",
            "Episode: 307   reward: -27.861158950894556\n",
            "Episode: 308   reward: -9.59056741478055\n",
            "Episode: 309   reward: -16.12326928296282\n",
            "Episode: 310   reward: -22.427192172518073\n",
            "Episode: 311   reward: -8.48696470339342\n",
            "Episode: 312   reward: -14.27920363324084\n",
            "Episode: 313   reward: -32.70948673894983\n",
            "Episode: 314   reward: -22.245778909490298\n",
            "Episode: 315   reward: -7.157890423421291\n",
            "Episode: 316   reward: -5.774057334588293\n",
            "Episode: 317   reward: -37.578551850094115\n",
            "Episode: 318   reward: -28.178562692391367\n",
            "Episode: 319   reward: -24.07746719164369\n",
            "Episode: 320   reward: -50.15227244972055\n",
            "Episode: 321   reward: -63.668753252043146\n",
            "Episode: 322   reward: -12.369261038743508\n",
            "Episode: 323   reward: -40.01085853687447\n",
            "Episode: 324   reward: -38.94526877660994\n",
            "Episode: 325   reward: -14.124263324735098\n",
            "Episode: 326   reward: -44.343308418993345\n",
            "Episode: 327   reward: -18.71634530399184\n",
            "Episode: 328   reward: -17.802165405427704\n",
            "Episode: 329   reward: -10.046619338228123\n",
            "Episode: 330   reward: -10.790329885002437\n",
            "Episode: 331   reward: -16.608534870142492\n",
            "Episode: 332   reward: -26.59434849687078\n",
            "Episode: 333   reward: -19.708564555521033\n",
            "Episode: 334   reward: -7.760130245071812\n",
            "Episode: 335   reward: -22.037269670630586\n",
            "Episode: 336   reward: -14.3257849470071\n",
            "Episode: 337   reward: -11.137380438274606\n",
            "Episode: 338   reward: -9.628009720825903\n",
            "Episode: 339   reward: -4.202493683979285\n",
            "Episode: 340   reward: -19.91406313283013\n",
            "Episode: 341   reward: -16.71893740662235\n",
            "Episode: 342   reward: -15.808825640948527\n",
            "Episode: 343   reward: -18.376007466485436\n",
            "Episode: 344   reward: -16.109243414743815\n",
            "Episode: 345   reward: -11.49894401941889\n",
            "Episode: 346   reward: -53.36619421941256\n",
            "Episode: 347   reward: -18.08506837864466\n",
            "Episode: 348   reward: -24.582290929440255\n",
            "Episode: 349   reward: -23.99270893713316\n",
            "Episode: 350   reward: -30.103657065688157\n",
            "Episode: 351   reward: -15.834917720409301\n",
            "Episode: 352   reward: -9.682741457855053\n",
            "Episode: 353   reward: -14.542596110729866\n",
            "Episode: 354   reward: -54.89561155190252\n",
            "Episode: 355   reward: -22.85702086699573\n",
            "Episode: 356   reward: -14.025080132823689\n",
            "Episode: 357   reward: -10.470052068839397\n",
            "Episode: 358   reward: -11.166317742422235\n",
            "Episode: 359   reward: -12.21624821901432\n",
            "Episode: 360   reward: -15.54616041328458\n",
            "Episode: 361   reward: -10.859723492915348\n",
            "Episode: 362   reward: -14.180848078128856\n",
            "Episode: 363   reward: -7.195210891932412\n",
            "Episode: 364   reward: -4.970618821135845\n",
            "Episode: 365   reward: -27.864768731614017\n",
            "Episode: 366   reward: -27.30736944653777\n",
            "Episode: 367   reward: -17.920860557366794\n",
            "Episode: 368   reward: -16.999839070170065\n",
            "Episode: 369   reward: -9.686684582677742\n",
            "Episode: 370   reward: -22.173225436601356\n",
            "Episode: 371   reward: -9.81912052259196\n",
            "Episode: 372   reward: -10.628271477108605\n",
            "Episode: 373   reward: -5.9297343280607855\n",
            "Episode: 374   reward: -9.118794261500335\n",
            "Episode: 375   reward: -10.08748021964496\n",
            "Episode: 376   reward: -11.432980395990468\n",
            "Episode: 377   reward: -7.884312645911561\n",
            "Episode: 378   reward: -13.49373748158861\n",
            "Episode: 379   reward: -8.5856760800479\n",
            "Episode: 380   reward: -10.866381650672963\n",
            "Episode: 381   reward: -6.839130439562002\n",
            "Episode: 382   reward: -2.8978178359387448\n",
            "Episode: 383   reward: -10.574683552335296\n",
            "Episode: 384   reward: -9.877203741163864\n",
            "Episode: 385   reward: -4.627021730855561\n",
            "Episode: 386   reward: -3.8984730709231106\n",
            "Episode: 387   reward: -3.9132331464317573\n",
            "Episode: 388   reward: -7.444507922859861\n",
            "Episode: 389   reward: -11.525861405971659\n",
            "Episode: 390   reward: -2.9814488525830343\n",
            "Episode: 391   reward: -10.976894076523898\n",
            "Episode: 392   reward: -9.430173348932824\n",
            "Episode: 393   reward: -8.051996498285021\n",
            "Episode: 394   reward: -19.879794043464162\n",
            "Episode: 395   reward: -10.166592926049097\n",
            "Episode: 396   reward: -15.895461106388709\n",
            "Episode: 397   reward: -2.217214386458452\n",
            "Episode: 398   reward: -7.803968156309603\n",
            "Episode: 399   reward: -26.55632262304189\n",
            "Episode: 400   reward: -31.994309505507914\n",
            "Episode: 401   reward: -17.26080362514454\n",
            "Episode: 402   reward: -7.396661244660536\n",
            "Episode: 403   reward: -8.78715674121967\n",
            "Episode: 404   reward: -15.183422240616263\n",
            "Episode: 405   reward: -6.682887028927592\n",
            "Episode: 406   reward: -13.737230246489043\n",
            "Episode: 407   reward: -11.578433896932493\n",
            "Episode: 408   reward: -6.88849207103147\n",
            "Episode: 409   reward: -6.345601740138264\n",
            "Episode: 410   reward: -8.771996231230572\n",
            "Episode: 411   reward: -8.962159043309448\n",
            "Episode: 412   reward: -10.422408323269988\n",
            "Episode: 413   reward: -8.020912365117525\n",
            "Episode: 414   reward: -19.619858542651283\n",
            "Episode: 415   reward: -2.6234996047281487\n",
            "Episode: 416   reward: -9.642457578442501\n",
            "Episode: 417   reward: -25.260720203507006\n",
            "Episode: 418   reward: -5.342857313898807\n",
            "Episode: 419   reward: -14.613864888110923\n",
            "Episode: 420   reward: -8.990994163008237\n",
            "Episode: 421   reward: -1.2433213470730786\n",
            "Episode: 422   reward: -28.25494366488783\n",
            "Episode: 423   reward: -35.05600459894293\n",
            "Episode: 424   reward: -8.679847417897983\n",
            "Episode: 425   reward: -20.513819933797926\n",
            "Episode: 426   reward: -6.2446050104826885\n",
            "Episode: 427   reward: -10.305623573447024\n",
            "Episode: 428   reward: -31.19308647613852\n",
            "Episode: 429   reward: -7.369300415837346\n",
            "Episode: 430   reward: -9.656792417582032\n",
            "Episode: 431   reward: -9.390241880400213\n",
            "Episode: 432   reward: -12.233418866883087\n",
            "Episode: 433   reward: -6.643561224123954\n",
            "Episode: 434   reward: -5.496456094368371\n",
            "Episode: 435   reward: -27.419930091325014\n",
            "Episode: 436   reward: -9.078995388594048\n",
            "Episode: 437   reward: -5.731268659322194\n",
            "Episode: 438   reward: -6.214960890784539\n",
            "Episode: 439   reward: -7.8502318710210774\n",
            "Episode: 440   reward: -25.331389644447995\n",
            "Episode: 441   reward: -13.91521165159108\n",
            "Episode: 442   reward: -1.4958970680033385\n",
            "Episode: 443   reward: -5.850383014945906\n",
            "Episode: 444   reward: -6.78144325752222\n",
            "Episode: 445   reward: -12.514645170420701\n",
            "Episode: 446   reward: -2.2316499612226157\n",
            "Episode: 447   reward: -7.728740332965189\n",
            "Episode: 448   reward: -12.039684881683389\n",
            "Episode: 449   reward: -2.5582062025457617\n",
            "Episode: 450   reward: -13.30557787293747\n",
            "Episode: 451   reward: -1.063762685843399\n",
            "Episode: 452   reward: -5.757822108715425\n",
            "Episode: 453   reward: -7.803037235484009\n",
            "Episode: 454   reward: -6.803336114266915\n",
            "Episode: 455   reward: -8.986737924743068\n",
            "Episode: 456   reward: -14.946689730646206\n",
            "Episode: 457   reward: -2.7364439900180155\n",
            "Episode: 458   reward: -9.820049637316211\n",
            "Episode: 459   reward: -4.848754897508443\n",
            "Episode: 460   reward: -10.835583968706953\n",
            "Episode: 461   reward: -8.417895387002968\n",
            "Episode: 462   reward: -7.096690302033841\n",
            "Episode: 463   reward: -20.544693938976636\n",
            "Episode: 464   reward: -39.65257176611483\n",
            "Episode: 465   reward: -9.165635862550683\n",
            "Episode: 466   reward: -42.10484991076143\n",
            "Episode: 467   reward: -28.640860130875165\n",
            "Episode: 468   reward: -11.730368768095204\n",
            "Episode: 469   reward: -10.056582639303887\n",
            "Episode: 470   reward: -6.62764591597328\n",
            "Episode: 471   reward: -18.73736392239191\n",
            "Episode: 472   reward: -30.912595093659686\n",
            "Episode: 473   reward: -5.183005641323767\n",
            "Episode: 474   reward: -1.504191789970602\n",
            "Episode: 475   reward: -6.5094647481225625\n",
            "Episode: 476   reward: -6.817900234763459\n",
            "Episode: 477   reward: -14.306129821031844\n",
            "Episode: 478   reward: -7.994701878252159\n",
            "Episode: 479   reward: -12.294783876982592\n",
            "Episode: 480   reward: -5.498545084619969\n",
            "Episode: 481   reward: -4.253217941604269\n",
            "Episode: 482   reward: -15.417980927871122\n",
            "Episode: 483   reward: -4.477244331557565\n",
            "Episode: 484   reward: -7.134240749061541\n",
            "Episode: 485   reward: -3.6828778102646855\n",
            "Episode: 486   reward: -11.761575422152193\n",
            "Episode: 487   reward: -3.359080625208975\n",
            "Episode: 488   reward: -2.3649371034074544\n",
            "Episode: 489   reward: -14.779561809121708\n",
            "Episode: 490   reward: -2.4147011997948686\n",
            "Episode: 491   reward: -7.310441599994588\n",
            "Episode: 492   reward: -1.4306514281606417\n",
            "Episode: 493   reward: -9.301299529321154\n",
            "Episode: 494   reward: -15.729144340728775\n",
            "Episode: 495   reward: -1.90228443334402\n",
            "Episode: 496   reward: -10.653016158125785\n",
            "Episode: 497   reward: -29.053106547452334\n",
            "Episode: 498   reward: -4.616849640597556\n",
            "Episode: 499   reward: -14.322747753363904\n",
            "Episode: 500   reward: -2.463393547062385\n",
            "model saved to models/2023-05-03_03-32-40/q_network_ep_0500.pth\n",
            "\n",
            "Episode: 501   reward: -1.6593218783326205\n",
            "Episode: 502   reward: -5.6314333561693735\n",
            "Episode: 503   reward: -15.656807306401337\n",
            "Episode: 504   reward: -4.691161149530087\n",
            "Episode: 505   reward: -13.069282503591726\n",
            "Episode: 506   reward: -4.6658383574666376\n",
            "Episode: 507   reward: -2.147144335558653\n",
            "Episode: 508   reward: -6.022284861838025\n",
            "Episode: 509   reward: -2.948736448902724\n",
            "Episode: 510   reward: -6.5692993574304674\n",
            "Episode: 511   reward: -7.49473818430931\n",
            "Episode: 512   reward: -12.402977634024749\n",
            "Episode: 513   reward: -7.96020531507586\n",
            "Episode: 514   reward: -3.2892022592503203\n",
            "Episode: 515   reward: -5.597834173045395\n",
            "Episode: 516   reward: -3.062456553772988\n",
            "Episode: 517   reward: -5.349705672890262\n",
            "Episode: 518   reward: -6.296562092529085\n",
            "Episode: 519   reward: -24.459700139036542\n",
            "Episode: 520   reward: -3.586969378796045\n",
            "Episode: 521   reward: -17.829595648631148\n",
            "Episode: 522   reward: -6.314635458544885\n",
            "Episode: 523   reward: -3.291076904308681\n",
            "Episode: 524   reward: -7.392792543944296\n",
            "Episode: 525   reward: -2.3797310825425573\n",
            "Episode: 526   reward: -9.548707848639312\n",
            "Episode: 527   reward: -2.7336347559343332\n",
            "Episode: 528   reward: -3.7143670815413006\n",
            "Episode: 529   reward: -3.78667682721325\n",
            "Episode: 530   reward: -2.0043018107853645\n",
            "Episode: 531   reward: -5.325214147296468\n",
            "Episode: 532   reward: -6.457634186148104\n",
            "Episode: 533   reward: -4.478844546774089\n",
            "Episode: 534   reward: -21.194308360344355\n",
            "Episode: 535   reward: -4.716393251067248\n",
            "Episode: 536   reward: -6.0625177745308685\n",
            "Episode: 537   reward: -7.837325203810142\n",
            "Episode: 538   reward: -7.2393268838170215\n",
            "Episode: 539   reward: -9.931304388016207\n",
            "Episode: 540   reward: -4.04484988827492\n",
            "Episode: 541   reward: -6.7728852065172465\n",
            "Episode: 542   reward: -3.1350691300143074\n",
            "Episode: 543   reward: -5.6727971935922845\n",
            "Episode: 544   reward: -3.010755521999591\n",
            "Episode: 545   reward: -2.1412833127062862\n",
            "Episode: 546   reward: -26.747160169107723\n",
            "Episode: 547   reward: -2.4117072616336053\n",
            "Episode: 548   reward: -8.857463615361489\n",
            "Episode: 549   reward: -3.687538121452279\n",
            "Episode: 550   reward: -7.724789825230564\n",
            "Episode: 551   reward: -14.337526394665042\n",
            "Episode: 552   reward: -10.383113284532655\n",
            "Episode: 553   reward: -2.7932417266885947\n",
            "Episode: 554   reward: -3.1643582615355186\n",
            "Episode: 555   reward: -10.028404293252922\n",
            "Episode: 556   reward: -1.0768702514291477\n",
            "Episode: 557   reward: -8.321365342288432\n",
            "Episode: 558   reward: -6.543356423663067\n",
            "Episode: 559   reward: -2.075681711428588\n",
            "Episode: 560   reward: -8.463031828309097\n",
            "Episode: 561   reward: -8.315410592513818\n",
            "Episode: 562   reward: -4.904916354690682\n",
            "Episode: 563   reward: -8.082199503965652\n",
            "Episode: 564   reward: -3.0166841295003968\n",
            "Episode: 565   reward: -5.027494965390193\n",
            "Episode: 566   reward: -8.30871630694135\n",
            "Episode: 567   reward: -8.442611714766127\n",
            "Episode: 568   reward: -6.640186973120608\n",
            "Episode: 569   reward: -2.3875001171485555\n",
            "Episode: 570   reward: -9.273068547738097\n",
            "Episode: 571   reward: -5.427425940483908\n",
            "Episode: 572   reward: -1.1119182165353325\n",
            "Episode: 573   reward: -2.1257431007879997\n",
            "Episode: 574   reward: -1.985319043523439\n",
            "Episode: 575   reward: -2.336148862602637\n",
            "Episode: 576   reward: -1.8249782192536037\n",
            "Episode: 577   reward: -3.794257913484583\n",
            "Episode: 578   reward: -2.8282644251029643\n",
            "Episode: 579   reward: -4.9223166750627065\n",
            "Episode: 580   reward: -2.376727271343595\n",
            "Episode: 581   reward: -5.529025010041778\n",
            "Episode: 582   reward: -7.014355029841603\n",
            "Episode: 583   reward: -2.2045454890614975\n",
            "Episode: 584   reward: -3.182072528689519\n",
            "Episode: 585   reward: -2.35541861814226\n",
            "Episode: 586   reward: -26.61053156616612\n",
            "Episode: 587   reward: -4.2014698706699205\n",
            "Episode: 588   reward: -3.642238208119081\n",
            "Episode: 589   reward: -2.529595088562392\n",
            "Episode: 590   reward: -9.929396982562144\n",
            "Episode: 591   reward: -3.629791262386861\n",
            "Episode: 592   reward: -3.4257445031629974\n",
            "Episode: 593   reward: -3.827477599534803\n",
            "Episode: 594   reward: -5.420879346147858\n",
            "Episode: 595   reward: -2.468988715075068\n",
            "Episode: 596   reward: -9.01805637564104\n",
            "Episode: 597   reward: -3.9250454561923696\n",
            "Episode: 598   reward: -5.765862485960305\n",
            "Episode: 599   reward: -2.976223528319093\n",
            "Episode: 600   reward: -2.50002248074741\n",
            "Episode: 601   reward: -3.6205381883056336\n",
            "Episode: 602   reward: -3.0118282314495426\n",
            "Episode: 603   reward: -8.473374995601846\n",
            "Episode: 604   reward: -4.9563891979023555\n",
            "Episode: 605   reward: -4.944433973063449\n",
            "Episode: 606   reward: -2.2861586314002937\n",
            "Episode: 607   reward: -4.74473828961284\n",
            "Episode: 608   reward: -8.775758129473168\n",
            "Episode: 609   reward: -1.834247090025516\n",
            "Episode: 610   reward: -7.227818850611111\n",
            "Episode: 611   reward: -7.046649774633836\n",
            "Episode: 612   reward: -2.545682893529431\n",
            "Episode: 613   reward: -6.783674840632123\n",
            "Episode: 614   reward: -4.650568189331332\n",
            "Episode: 615   reward: -8.084341469879403\n",
            "Episode: 616   reward: -5.948437344605038\n",
            "Episode: 617   reward: -3.1703954126101426\n",
            "Episode: 618   reward: -8.309967194860574\n",
            "Episode: 619   reward: -4.4217087144612695\n",
            "Episode: 620   reward: -4.324592849727691\n",
            "Episode: 621   reward: -10.003248427821303\n",
            "Episode: 622   reward: -4.9371146774856385\n",
            "Episode: 623   reward: -21.902612841535955\n",
            "Episode: 624   reward: -28.57962062211482\n",
            "Episode: 625   reward: -16.91690612166517\n",
            "Episode: 626   reward: -8.721300722525566\n",
            "Episode: 627   reward: -5.86808597645528\n",
            "Episode: 628   reward: -4.91193101211613\n",
            "Episode: 629   reward: -3.7181345875227225\n",
            "Episode: 630   reward: -21.430382967211184\n",
            "Episode: 631   reward: -6.463565245242361\n",
            "Episode: 632   reward: -7.919161774051597\n",
            "Episode: 633   reward: -6.411229011582092\n",
            "Episode: 634   reward: -3.4137691561814663\n",
            "Episode: 635   reward: -4.252925197783633\n",
            "Episode: 636   reward: -3.008283757315619\n",
            "Episode: 637   reward: -6.338497366951897\n",
            "Episode: 638   reward: -4.9025499780559905\n",
            "Episode: 639   reward: -3.927611729527509\n",
            "Episode: 640   reward: -2.5802914027691344\n",
            "Episode: 641   reward: -5.223203212649278\n",
            "Episode: 642   reward: -6.414176725959356\n",
            "Episode: 643   reward: -7.112429520920849\n",
            "Episode: 644   reward: -1.704134222858365\n",
            "Episode: 645   reward: -2.954768753434867\n",
            "Episode: 646   reward: -5.452300853384478\n",
            "Episode: 647   reward: -1.3606012770237732\n",
            "Episode: 648   reward: -9.074069952905257\n",
            "Episode: 649   reward: -4.25228607145841\n",
            "Episode: 650   reward: -9.035156945786015\n",
            "Episode: 651   reward: -2.8449080361517964\n",
            "Episode: 652   reward: -7.48908088624027\n",
            "Episode: 653   reward: -1.8082221180914642\n",
            "Episode: 654   reward: -8.554944234054423\n",
            "Episode: 655   reward: -2.9921982055301313\n",
            "Episode: 656   reward: -4.658801514263938\n",
            "Episode: 657   reward: -5.777656995607595\n",
            "Episode: 658   reward: -2.5755031591580013\n",
            "Episode: 659   reward: -6.506179893590839\n",
            "Episode: 660   reward: -2.1979873898502955\n",
            "Episode: 661   reward: -3.198955115485898\n",
            "Episode: 662   reward: -2.253776668467771\n",
            "Episode: 663   reward: -3.039493730054001\n",
            "Episode: 664   reward: -4.800902276120904\n",
            "Episode: 665   reward: -2.419262821951934\n",
            "Episode: 666   reward: -2.2837977557929006\n",
            "Episode: 667   reward: -2.8615687436103596\n",
            "Episode: 668   reward: -3.4124972084689364\n",
            "Episode: 669   reward: -2.858359566253018\n",
            "Episode: 670   reward: -15.496048685475891\n",
            "Episode: 671   reward: -2.6742760185101915\n",
            "Episode: 672   reward: -5.1908659739448515\n",
            "Episode: 673   reward: -2.3095639059201813\n",
            "Episode: 674   reward: -2.692277616383337\n",
            "Episode: 675   reward: -2.819720492461771\n",
            "Episode: 676   reward: -2.9811478751274163\n",
            "Episode: 677   reward: -6.660394351439971\n",
            "Episode: 678   reward: -14.13773511331054\n",
            "Episode: 679   reward: -6.68202135477621\n",
            "Episode: 680   reward: -7.635562118893777\n",
            "Episode: 681   reward: -13.255944339684927\n",
            "Episode: 682   reward: -13.069617209880345\n",
            "Episode: 683   reward: -3.7062854750259024\n",
            "Episode: 684   reward: -7.7079681487901945\n",
            "Episode: 685   reward: -4.0460930059167355\n",
            "Episode: 686   reward: -1.87664238910968\n",
            "Episode: 687   reward: -5.35011576872267\n",
            "Episode: 688   reward: -3.205416133276001\n",
            "Episode: 689   reward: -7.024538192835692\n",
            "Episode: 690   reward: -18.070969585874078\n",
            "Episode: 691   reward: -10.789812187516892\n",
            "Episode: 692   reward: -2.686056600286399\n",
            "Episode: 693   reward: -3.4457767298731214\n",
            "Episode: 694   reward: -2.270867559880738\n",
            "Episode: 695   reward: -1.840243289610447\n",
            "Episode: 696   reward: -17.352364334934546\n",
            "Episode: 697   reward: -3.8507456822955484\n",
            "Episode: 698   reward: -4.597577748896321\n",
            "Episode: 699   reward: -8.786844833532077\n",
            "Episode: 700   reward: -17.631815143485273\n",
            "Episode: 701   reward: -11.10475612625836\n",
            "Episode: 702   reward: -13.222456948237337\n",
            "Episode: 703   reward: -2.391468284304295\n",
            "Episode: 704   reward: -4.298582974386806\n",
            "Episode: 705   reward: -2.873805126921781\n",
            "Episode: 706   reward: -4.209087210287499\n",
            "Episode: 707   reward: -4.569942414915197\n",
            "Episode: 708   reward: -2.2702906556721456\n",
            "Episode: 709   reward: -9.691443690107901\n",
            "Episode: 710   reward: -12.501237221855506\n",
            "Episode: 711   reward: -16.44258013801232\n",
            "Episode: 712   reward: -3.7436255037120265\n",
            "Episode: 713   reward: -2.6633201626679077\n",
            "Episode: 714   reward: -12.586196162521668\n",
            "Episode: 715   reward: -6.1938586962216595\n",
            "Episode: 716   reward: -2.3018657379645324\n",
            "Episode: 717   reward: -19.680160199550716\n",
            "Episode: 718   reward: -7.979455505818666\n",
            "Episode: 719   reward: -2.8854830217740512\n",
            "Episode: 720   reward: -2.449512699252198\n",
            "Episode: 721   reward: -1.861082422297647\n",
            "Episode: 722   reward: -28.88767730344483\n",
            "Episode: 723   reward: -20.64514796485165\n",
            "Episode: 724   reward: -2.0394606938394615\n",
            "Episode: 725   reward: -3.1987261286180697\n",
            "Episode: 726   reward: -13.688548252594655\n",
            "Episode: 727   reward: -8.482648993362494\n",
            "Episode: 728   reward: -4.906883375672906\n",
            "Episode: 729   reward: -2.5974170901222675\n",
            "Episode: 730   reward: -11.635661106710312\n",
            "Episode: 731   reward: -3.8918075856988446\n",
            "Episode: 732   reward: -2.7257995702590936\n",
            "Episode: 733   reward: -2.78165162353041\n",
            "Episode: 734   reward: -7.082002150017803\n",
            "Episode: 735   reward: -4.980724485893853\n",
            "Episode: 736   reward: -5.056894503503136\n",
            "Episode: 737   reward: -3.8843074818196737\n",
            "Episode: 738   reward: -3.7787253787329926\n",
            "Episode: 739   reward: -1.661135618367892\n",
            "Episode: 740   reward: -2.9223812746626057\n",
            "Episode: 741   reward: -4.6263476259323175\n",
            "Episode: 742   reward: -4.255007294134481\n",
            "Episode: 743   reward: -2.379288326026094\n",
            "Episode: 744   reward: -3.124787999453883\n",
            "Episode: 745   reward: -1.5324259657554966\n",
            "Episode: 746   reward: -7.550211009286708\n",
            "Episode: 747   reward: -5.5169097759194825\n",
            "Episode: 748   reward: -1.8493999712437394\n",
            "Episode: 749   reward: -6.02515173461862\n",
            "Episode: 750   reward: -6.2700132886502145\n",
            "Episode: 751   reward: -16.784668133674217\n",
            "Episode: 752   reward: -0.9613257910632637\n",
            "Episode: 753   reward: -8.093714751365729\n",
            "Episode: 754   reward: -16.650880342263704\n",
            "Episode: 755   reward: -7.0503403348490545\n",
            "Episode: 756   reward: -3.3562889778650904\n",
            "Episode: 757   reward: -3.121053622832413\n",
            "Episode: 758   reward: -4.643328415697537\n",
            "Episode: 759   reward: -7.295087300613373\n",
            "Episode: 760   reward: -2.45264886230521\n",
            "Episode: 761   reward: -2.7270410990689444\n",
            "Episode: 762   reward: -3.004621084967236\n",
            "Episode: 763   reward: -2.4183649486250154\n",
            "Episode: 764   reward: -1.7155006084456477\n",
            "Episode: 765   reward: -4.6168496554505145\n",
            "Episode: 766   reward: -3.301558371262861\n",
            "Episode: 767   reward: -2.495831218601452\n",
            "Episode: 768   reward: -2.5637740250045424\n",
            "Episode: 769   reward: -2.7766080878881865\n",
            "Episode: 770   reward: -7.938676641192627\n",
            "Episode: 771   reward: -2.1664466083303626\n",
            "Episode: 772   reward: -3.701926020920455\n",
            "Episode: 773   reward: -3.6651484732777218\n",
            "Episode: 774   reward: -5.657742115377896\n",
            "Episode: 775   reward: -4.531147868593305\n",
            "Episode: 776   reward: -2.001344172821285\n",
            "Episode: 777   reward: -2.2144613819135865\n",
            "Episode: 778   reward: -2.44053903146111\n",
            "Episode: 779   reward: -1.2226010660558493\n",
            "Episode: 780   reward: -1.2802821159552873\n",
            "Episode: 781   reward: -4.392921668655442\n",
            "Episode: 782   reward: -2.1729952149822265\n",
            "Episode: 783   reward: -3.0654741300602804\n",
            "Episode: 784   reward: -3.6544211324650955\n",
            "Episode: 785   reward: -5.265357426867826\n",
            "Episode: 786   reward: -2.2781708729130905\n",
            "Episode: 787   reward: -4.339148162111296\n",
            "Episode: 788   reward: -5.121386003221691\n",
            "Episode: 789   reward: -10.867615700170305\n",
            "Episode: 790   reward: -3.5966730085145358\n",
            "Episode: 791   reward: -2.600836810301515\n",
            "Episode: 792   reward: -1.4207420908307098\n",
            "Episode: 793   reward: -2.8164563719610993\n",
            "Episode: 794   reward: -4.437105725729301\n",
            "Episode: 795   reward: -2.736375901924121\n",
            "Episode: 796   reward: -5.398004812973921\n",
            "Episode: 797   reward: -7.231743671363141\n",
            "Episode: 798   reward: -3.9957326031769718\n",
            "Episode: 799   reward: -2.5746305866711294\n",
            "Episode: 800   reward: -3.966534689886523\n",
            "Episode: 801   reward: -2.272570162250514\n",
            "Episode: 802   reward: -3.6843169836976446\n",
            "Episode: 803   reward: -11.321196333502481\n",
            "Episode: 804   reward: -1.3828719337601032\n",
            "Episode: 805   reward: -0.9494360718702338\n",
            "Episode: 806   reward: -1.578100762198426\n",
            "Episode: 807   reward: -3.340945918253566\n",
            "Episode: 808   reward: -7.232924611357066\n",
            "Episode: 809   reward: -1.288304013004255\n",
            "Episode: 810   reward: -4.901168648880316\n",
            "Episode: 811   reward: -2.483700252218172\n",
            "Episode: 812   reward: -6.019963764594368\n",
            "Episode: 813   reward: -0.9594351809972337\n",
            "Episode: 814   reward: -6.032549748507229\n",
            "Episode: 815   reward: -2.5441515051049475\n",
            "Episode: 816   reward: -3.283074526445616\n",
            "Episode: 817   reward: -3.539177378586296\n",
            "Episode: 818   reward: -8.38934063070194\n",
            "Episode: 819   reward: -4.136719970500693\n",
            "Episode: 820   reward: -2.9946808940460428\n",
            "Episode: 821   reward: -1.3013231501239801\n",
            "Episode: 822   reward: -3.363570910707286\n",
            "Episode: 823   reward: -8.39554201833061\n",
            "Episode: 824   reward: -3.0411967438654064\n",
            "Episode: 825   reward: -16.585970256571297\n",
            "Episode: 826   reward: -5.195830537764352\n",
            "Episode: 827   reward: -2.7547527630259587\n",
            "Episode: 828   reward: -9.23189191348463\n",
            "Episode: 829   reward: -7.112949046329012\n",
            "Episode: 830   reward: -9.16433252361078\n",
            "Episode: 831   reward: -2.0998406908127785\n",
            "Episode: 832   reward: -0.5270511802354894\n",
            "Episode: 833   reward: -7.894606732581149\n",
            "Episode: 834   reward: -0.6283700728837746\n",
            "Episode: 835   reward: -6.535826453140142\n",
            "Episode: 836   reward: -1.3496212616144911\n",
            "Episode: 837   reward: -4.9553836014116985\n",
            "Episode: 838   reward: -2.597045525042876\n",
            "Episode: 839   reward: -4.24385716631236\n",
            "Episode: 840   reward: -0.9866510787997529\n",
            "Episode: 841   reward: -4.16699367652104\n",
            "Episode: 842   reward: -2.446105460858615\n",
            "Episode: 843   reward: -5.874384471873555\n",
            "Episode: 844   reward: -3.5083492554741014\n",
            "Episode: 845   reward: -3.203346880121166\n",
            "Episode: 846   reward: -5.227970291422067\n",
            "Episode: 847   reward: -1.650542094629277\n",
            "Episode: 848   reward: -3.4043172889551987\n",
            "Episode: 849   reward: -3.074904581319018\n",
            "Episode: 850   reward: -2.0656391528039237\n",
            "Episode: 851   reward: -3.0759024396017782\n",
            "Episode: 852   reward: -3.6534553723585717\n",
            "Episode: 853   reward: -2.013614560403075\n",
            "Episode: 854   reward: -6.055108766345484\n",
            "Episode: 855   reward: -2.6974088056966763\n",
            "Episode: 856   reward: -5.738111778171693\n",
            "Episode: 857   reward: -4.064911368496123\n",
            "Episode: 858   reward: -5.119048178544368\n",
            "Episode: 859   reward: -2.6678154869910315\n",
            "Episode: 860   reward: -7.264121487146928\n",
            "Episode: 861   reward: -1.7571879677396953\n",
            "Episode: 862   reward: -4.911768335353256\n",
            "Episode: 863   reward: -1.5575598246687368\n",
            "Episode: 864   reward: -1.7335166296154538\n",
            "Episode: 865   reward: -1.6621358127914383\n",
            "Episode: 866   reward: -7.250565347805805\n",
            "Episode: 867   reward: -2.5897105282649644\n",
            "Episode: 868   reward: -4.34445714798326\n",
            "Episode: 869   reward: -7.122924673089901\n",
            "Episode: 870   reward: -0.9018233108014403\n",
            "Episode: 871   reward: -3.942736584360917\n",
            "Episode: 872   reward: -5.997545131497732\n",
            "Episode: 873   reward: -2.9352131979966587\n",
            "Episode: 874   reward: -2.5919022998912284\n",
            "Episode: 875   reward: -4.256929019543581\n",
            "Episode: 876   reward: -4.326815420487054\n",
            "Episode: 877   reward: -2.568855601221176\n",
            "Episode: 878   reward: -3.9297598362005677\n",
            "Episode: 879   reward: -2.3512176158097096\n",
            "Episode: 880   reward: -0.9155914306024222\n",
            "Episode: 881   reward: -4.385579200603529\n",
            "Episode: 882   reward: -1.6971324160729864\n",
            "Episode: 883   reward: -1.2817366604856184\n",
            "Episode: 884   reward: -1.2754828808144703\n",
            "Episode: 885   reward: -5.555701234101617\n",
            "Episode: 886   reward: -0.8657518161582499\n",
            "Episode: 887   reward: -5.3193636893568\n",
            "Episode: 888   reward: -4.33167933978218\n",
            "Episode: 889   reward: -2.333420835913524\n",
            "Episode: 890   reward: -1.4702782953837328\n",
            "Episode: 891   reward: -1.982333229852389\n",
            "Episode: 892   reward: -1.8752686861960444\n",
            "Episode: 893   reward: -1.5006986826411897\n",
            "Episode: 894   reward: -1.3396243906194008\n",
            "Episode: 895   reward: -1.1030286838674421\n",
            "Episode: 896   reward: -1.2941174434983018\n",
            "Episode: 897   reward: -4.603501038341235\n",
            "Episode: 898   reward: -2.8520865331042025\n",
            "Episode: 899   reward: -4.788092553747974\n",
            "Episode: 900   reward: -2.003261310473346\n",
            "Episode: 901   reward: -1.5867874570978333\n",
            "Episode: 902   reward: -2.836645886792458\n",
            "Episode: 903   reward: -3.697661340102188\n",
            "Episode: 904   reward: -1.670755344334297\n",
            "Episode: 905   reward: -2.744417279920956\n",
            "Episode: 906   reward: -4.165931079141047\n",
            "Episode: 907   reward: -2.2241143334437607\n",
            "Episode: 908   reward: -5.615473937361963\n",
            "Episode: 909   reward: -4.827438629920412\n",
            "Episode: 910   reward: -6.464524851436012\n",
            "Episode: 911   reward: -2.227164348072327\n",
            "Episode: 912   reward: -2.03869886384869\n",
            "Episode: 913   reward: -4.79364823964395\n",
            "Episode: 914   reward: -2.1377588137182713\n",
            "Episode: 915   reward: -7.2254206229063564\n",
            "Episode: 916   reward: -8.316342934519666\n",
            "Episode: 917   reward: -3.9936073002864614\n",
            "Episode: 918   reward: -4.027503678903875\n",
            "Episode: 919   reward: -6.187760309388312\n",
            "Episode: 920   reward: -5.665219054261675\n",
            "Episode: 921   reward: -3.968932082963159\n",
            "Episode: 922   reward: -1.7635605311935267\n",
            "Episode: 923   reward: -4.249033577026105\n",
            "Episode: 924   reward: -2.063652618191793\n",
            "Episode: 925   reward: -2.7737300706845014\n",
            "Episode: 926   reward: -6.279831746961522\n",
            "Episode: 927   reward: -1.4558904871197167\n",
            "Episode: 928   reward: -1.401338898991555\n",
            "Episode: 929   reward: -1.575331250502008\n",
            "Episode: 930   reward: -4.621910974880638\n",
            "Episode: 931   reward: -3.277805574636883\n",
            "Episode: 932   reward: -1.2725749260530494\n",
            "Episode: 933   reward: -3.7940848130860965\n",
            "Episode: 934   reward: -3.2393605802498433\n",
            "Episode: 935   reward: -5.373877475223889\n",
            "Episode: 936   reward: -3.0699176455087804\n",
            "Episode: 937   reward: -1.7719040204396106\n",
            "Episode: 938   reward: -2.7551877911270815\n",
            "Episode: 939   reward: -6.146363895479103\n",
            "Episode: 940   reward: -2.153212988191325\n",
            "Episode: 941   reward: -2.8562112267097737\n",
            "Episode: 942   reward: -3.2110435952799348\n",
            "Episode: 943   reward: -2.9942113379045043\n",
            "Episode: 944   reward: -2.7684536668869937\n",
            "Episode: 945   reward: -3.4434992746270763\n",
            "Episode: 946   reward: -1.3579614578154828\n",
            "Episode: 947   reward: -0.576034380840307\n",
            "Episode: 948   reward: -3.1234676540346755\n",
            "Episode: 949   reward: -2.852965894207845\n",
            "Episode: 950   reward: -4.929340036512149\n",
            "Episode: 951   reward: -6.361497049273803\n",
            "Episode: 952   reward: -5.921857730056433\n",
            "Episode: 953   reward: -3.613081360563407\n",
            "Episode: 954   reward: -4.5181190382472245\n",
            "Episode: 955   reward: -4.907293897497207\n",
            "Episode: 956   reward: -1.5131874410985127\n",
            "Episode: 957   reward: -2.2903544559951374\n",
            "Episode: 958   reward: -4.218732512064451\n",
            "Episode: 959   reward: -3.744034305651807\n",
            "Episode: 960   reward: -3.8406687945817937\n",
            "Episode: 961   reward: -4.704037087426408\n",
            "Episode: 962   reward: -1.4745285307367522\n",
            "Episode: 963   reward: -1.3988773838082555\n",
            "Episode: 964   reward: -2.918902177991701\n",
            "Episode: 965   reward: -4.122914703581849\n",
            "Episode: 966   reward: -3.767082613493545\n",
            "Episode: 967   reward: -1.4303514642854551\n",
            "Episode: 968   reward: -3.616506832568131\n",
            "Episode: 969   reward: -5.355661747310306\n",
            "Episode: 970   reward: -1.7830177120577884\n",
            "Episode: 971   reward: -3.429784232402163\n",
            "Episode: 972   reward: -3.2353600638899116\n",
            "Episode: 973   reward: -4.699761356118294\n",
            "Episode: 974   reward: -2.5220141578457023\n",
            "Episode: 975   reward: -1.526401425359765\n",
            "Episode: 976   reward: -2.2470761558089705\n",
            "Episode: 977   reward: -10.557510716780719\n",
            "Episode: 978   reward: -1.4972533737281415\n",
            "Episode: 979   reward: -1.843950958110966\n",
            "Episode: 980   reward: -1.8264427614927268\n",
            "Episode: 981   reward: -4.82816452222611\n",
            "Episode: 982   reward: -2.240968393995347\n",
            "Episode: 983   reward: -4.8444578415890645\n",
            "Episode: 984   reward: -1.6686662854271226\n",
            "Episode: 985   reward: -3.5980726510581507\n",
            "Episode: 986   reward: -2.3995709110957404\n",
            "Episode: 987   reward: -2.5451771926315034\n",
            "Episode: 988   reward: -1.2064896774537353\n",
            "Episode: 989   reward: -3.5248868368440593\n",
            "Episode: 990   reward: -1.9868182058666333\n",
            "Episode: 991   reward: -6.751042390733332\n",
            "Episode: 992   reward: -2.153035830373079\n",
            "Episode: 993   reward: -5.852117626215061\n",
            "Episode: 994   reward: -2.5029109366980324\n",
            "Episode: 995   reward: -5.591093648278696\n",
            "Episode: 996   reward: -3.1145086728651212\n",
            "Episode: 997   reward: -2.1888565470479096\n",
            "Episode: 998   reward: -1.199912193149261\n",
            "Episode: 999   reward: -2.790659615696455\n",
            "Episode: 1000   reward: -7.342345132036447\n",
            "model saved to models/2023-05-03_03-38-43/q_network_ep_1000.pth\n",
            "\n",
            "Episode: 1001   reward: -3.9495419489599013\n",
            "Episode: 1002   reward: -5.046387154074434\n",
            "Episode: 1003   reward: -4.437127911002475\n",
            "Episode: 1004   reward: -11.632288429765316\n",
            "Episode: 1005   reward: -7.7773175916095525\n",
            "Episode: 1006   reward: -1.6708843179395563\n",
            "Episode: 1007   reward: -4.005005120820655\n",
            "Episode: 1008   reward: -4.182060010972585\n",
            "Episode: 1009   reward: -5.516521196213773\n",
            "Episode: 1010   reward: -3.026315444099901\n",
            "Episode: 1011   reward: -3.894104808404508\n",
            "Episode: 1012   reward: -13.219937366240824\n",
            "Episode: 1013   reward: -3.316299360733735\n",
            "Episode: 1014   reward: -2.3326612389977304\n",
            "Episode: 1015   reward: -1.1641287901601192\n",
            "Episode: 1016   reward: -6.352569135412224\n",
            "Episode: 1017   reward: -1.6724559699685544\n",
            "Episode: 1018   reward: -4.83730564818309\n",
            "Episode: 1019   reward: -1.173472525464507\n",
            "Episode: 1020   reward: -1.9227382367167964\n",
            "Episode: 1021   reward: -4.8973009648903485\n",
            "Episode: 1022   reward: -6.909035858569947\n",
            "Episode: 1023   reward: -1.7875664335840697\n",
            "Episode: 1024   reward: -20.01674274485565\n",
            "Episode: 1025   reward: -4.186924180490315\n",
            "Episode: 1026   reward: -4.172961930837417\n",
            "Episode: 1027   reward: -1.9801761969291318\n",
            "Episode: 1028   reward: -4.198796363449384\n",
            "Episode: 1029   reward: -1.7177051876586495\n",
            "Episode: 1030   reward: -2.71765768536362\n",
            "Episode: 1031   reward: -1.1979782076965453\n",
            "Episode: 1032   reward: -0.9856848148156895\n",
            "Episode: 1033   reward: -5.122431274740257\n",
            "Episode: 1034   reward: -1.050545884518971\n",
            "Episode: 1035   reward: -0.9077212146604513\n",
            "Episode: 1036   reward: -2.547245819938914\n",
            "Episode: 1037   reward: -2.3846240029093457\n",
            "Episode: 1038   reward: -3.751130409638398\n",
            "Episode: 1039   reward: -3.721992940899636\n",
            "Episode: 1040   reward: -8.467500620863902\n",
            "Episode: 1041   reward: -3.6051980673560906\n",
            "Episode: 1042   reward: -5.285415017348313\n",
            "Episode: 1043   reward: -2.678720726076391\n",
            "Episode: 1044   reward: -1.576587752952995\n",
            "Episode: 1045   reward: -9.406507568768065\n",
            "Episode: 1046   reward: -5.910590626383346\n",
            "Episode: 1047   reward: -1.3320907570011395\n",
            "Episode: 1048   reward: -4.954168797739558\n",
            "Episode: 1049   reward: -8.069202288327006\n",
            "Episode: 1050   reward: -7.497302624411911\n",
            "Episode: 1051   reward: -1.2329214038928518\n",
            "Episode: 1052   reward: -2.3880269390209636\n",
            "Episode: 1053   reward: -0.7563300492206084\n",
            "Episode: 1054   reward: -2.239536200738464\n",
            "Episode: 1055   reward: -2.604244755903919\n",
            "Episode: 1056   reward: -2.417995350920478\n",
            "Episode: 1057   reward: -2.9795753293634912\n",
            "Episode: 1058   reward: -9.48474171555986\n",
            "Episode: 1059   reward: -2.510858616918005\n",
            "Episode: 1060   reward: -3.2584411239942757\n",
            "Episode: 1061   reward: -4.101512573027574\n",
            "Episode: 1062   reward: -5.6952960054755035\n",
            "Episode: 1063   reward: -3.2562524874400114\n",
            "Episode: 1064   reward: -3.2944935912350326\n",
            "Episode: 1065   reward: -1.1793994473742389\n",
            "Episode: 1066   reward: -4.088608246521915\n",
            "Episode: 1067   reward: -2.1039279232046058\n",
            "Episode: 1068   reward: -3.802911512757472\n",
            "Episode: 1069   reward: -2.881919915765353\n",
            "Episode: 1070   reward: -1.1229634223297797\n",
            "Episode: 1071   reward: -26.46555559686052\n",
            "Episode: 1072   reward: -3.815920508160917\n",
            "Episode: 1073   reward: -2.6286535962222386\n",
            "Episode: 1074   reward: -3.818465779520792\n",
            "Episode: 1075   reward: -1.8467395629634888\n",
            "Episode: 1076   reward: -1.4004347016212595\n",
            "Episode: 1077   reward: -2.4020195976911216\n",
            "Episode: 1078   reward: -1.1501554385757664\n",
            "Episode: 1079   reward: -3.9109880784355573\n",
            "Episode: 1080   reward: -5.071638032569806\n",
            "Episode: 1081   reward: -6.174541632779697\n",
            "Episode: 1082   reward: -4.977406929321208\n",
            "Episode: 1083   reward: -7.889026691909029\n",
            "Episode: 1084   reward: -4.836794952322314\n",
            "Episode: 1085   reward: -1.9411711195653447\n",
            "Episode: 1086   reward: -1.0886113769111347\n",
            "Episode: 1087   reward: -2.062656068811874\n",
            "Episode: 1088   reward: -3.5335193044639195\n",
            "Episode: 1089   reward: -4.423241363761845\n",
            "Episode: 1090   reward: -1.8755152923997087\n",
            "Episode: 1091   reward: -5.932878024715567\n",
            "Episode: 1092   reward: -3.038708161996146\n",
            "Episode: 1093   reward: -1.9587162302050654\n",
            "Episode: 1094   reward: -9.055436153457064\n",
            "Episode: 1095   reward: -1.2154344619377788\n",
            "Episode: 1096   reward: -4.547173308182046\n",
            "Episode: 1097   reward: -0.9456950498558021\n",
            "Episode: 1098   reward: -3.484434173594032\n",
            "Episode: 1099   reward: -4.50490854485492\n",
            "Episode: 1100   reward: -1.8099244714889842\n",
            "Episode: 1101   reward: -5.54460818470361\n",
            "Episode: 1102   reward: -2.938331062060623\n",
            "Episode: 1103   reward: -3.318425491235246\n",
            "Episode: 1104   reward: -8.274002124772492\n",
            "Episode: 1105   reward: -3.164278552916831\n",
            "Episode: 1106   reward: -1.3555473206072757\n",
            "Episode: 1107   reward: -6.927055125585503\n",
            "Episode: 1108   reward: -0.969284108479421\n",
            "Episode: 1109   reward: -2.7493197606434157\n",
            "Episode: 1110   reward: -11.421820042369655\n",
            "Episode: 1111   reward: -0.9494382995346315\n",
            "Episode: 1112   reward: -5.325079923301848\n",
            "Episode: 1113   reward: -1.6216344458881962\n",
            "Episode: 1114   reward: -3.3815421958731045\n",
            "Episode: 1115   reward: -1.5321028520267637\n",
            "Episode: 1116   reward: -0.809010898689109\n",
            "Episode: 1117   reward: -1.498989814739496\n",
            "Episode: 1118   reward: -9.30260505045271\n",
            "Episode: 1119   reward: -10.89164056570397\n",
            "Episode: 1120   reward: -10.276658693870086\n",
            "Episode: 1121   reward: -1.0106863211104655\n",
            "Episode: 1122   reward: -3.7412273420457507\n",
            "Episode: 1123   reward: -0.6833962108129356\n",
            "Episode: 1124   reward: -7.930896640047244\n",
            "Episode: 1125   reward: -1.0360435876165441\n",
            "Episode: 1126   reward: -6.467698466515338\n",
            "Episode: 1127   reward: -2.032044789762405\n",
            "Episode: 1128   reward: -1.7466672826073542\n",
            "Episode: 1129   reward: -1.334652744908991\n",
            "Episode: 1130   reward: -3.0068184955001205\n",
            "Episode: 1131   reward: -1.5112821857938776\n",
            "Episode: 1132   reward: -1.669338112121514\n",
            "Episode: 1133   reward: -1.5791409879116545\n",
            "Episode: 1134   reward: -1.0360010291913528\n",
            "Episode: 1135   reward: -2.1756793785541713\n",
            "Episode: 1136   reward: -8.62469780732078\n",
            "Episode: 1137   reward: -1.5606723399260578\n",
            "Episode: 1138   reward: -8.63086283820797\n",
            "Episode: 1139   reward: -14.174386174919226\n",
            "Episode: 1140   reward: -1.2648869803361644\n",
            "Episode: 1141   reward: -5.311370711223746\n",
            "Episode: 1142   reward: -3.385846475861049\n",
            "Episode: 1143   reward: -11.84036732226411\n",
            "Episode: 1144   reward: -2.2986992985432884\n",
            "Episode: 1145   reward: -0.5842474370313988\n",
            "Episode: 1146   reward: -86.55045102901413\n",
            "Episode: 1147   reward: -7.255500342972293\n",
            "Episode: 1148   reward: -256.01822165187605\n",
            "Episode: 1149   reward: -2.068346935264749\n",
            "Episode: 1150   reward: -10.903446373931128\n",
            "Episode: 1151   reward: -5.7858781003314945\n",
            "Episode: 1152   reward: -2.350406507173362\n",
            "Episode: 1153   reward: -4.014897042291987\n",
            "Episode: 1154   reward: -12.615270781245936\n",
            "Episode: 1155   reward: -0.876400783787538\n",
            "Episode: 1156   reward: -1.6381808624153766\n",
            "Episode: 1157   reward: -4.581946432661803\n",
            "Episode: 1158   reward: -8.366849347517553\n",
            "Episode: 1159   reward: -13.351917628134336\n",
            "Episode: 1160   reward: -4.569803380724086\n",
            "Episode: 1161   reward: -0.6653894002431231\n",
            "Episode: 1162   reward: -361.99646605339217\n",
            "Episode: 1163   reward: -19.5802218540983\n",
            "Episode: 1164   reward: -1.606731869242852\n",
            "Episode: 1165   reward: -14.21313536667226\n",
            "Episode: 1166   reward: -2.028749419594954\n",
            "Episode: 1167   reward: -5.238845388636595\n",
            "Episode: 1168   reward: -8.529890466975218\n",
            "Episode: 1169   reward: -381.7750426826703\n",
            "Episode: 1170   reward: -431.9657688808807\n",
            "Episode: 1171   reward: -1.2182541574525152\n",
            "Episode: 1172   reward: -2.45476612986013\n",
            "Episode: 1173   reward: -7.796814728856718\n",
            "Episode: 1174   reward: -37.22027841298522\n",
            "Episode: 1175   reward: -386.9964709208826\n",
            "Episode: 1176   reward: -51.59460093387584\n",
            "Episode: 1177   reward: -340.2482151730584\n",
            "Episode: 1178   reward: -8.185686908880509\n",
            "Episode: 1179   reward: -2.790285599819155\n",
            "Episode: 1180   reward: -18.1358885907196\n",
            "Episode: 1181   reward: -9.829250816871058\n",
            "Episode: 1182   reward: -16.251853259771963\n",
            "Episode: 1183   reward: -0.5994287551031774\n",
            "Episode: 1184   reward: -5.542983232190925\n",
            "Episode: 1185   reward: -17.01072148087719\n",
            "Episode: 1186   reward: -0.8508951659191026\n",
            "Episode: 1187   reward: -10.040295009156864\n",
            "Episode: 1188   reward: -6.5364241465668345\n",
            "Episode: 1189   reward: -2.263676000268325\n",
            "Episode: 1190   reward: -5.082601468218203\n",
            "Episode: 1191   reward: -4.326778254339847\n",
            "Episode: 1192   reward: -4.696195720761103\n",
            "Episode: 1193   reward: -7.741533875819092\n",
            "Episode: 1194   reward: -21.401189961927653\n",
            "Episode: 1195   reward: -5.594986760594355\n",
            "Episode: 1196   reward: -7.937059749941293\n",
            "Episode: 1197   reward: -0.4966981720140756\n",
            "Episode: 1198   reward: -1.3348725713293155\n",
            "Episode: 1199   reward: -0.47460324082522415\n",
            "Episode: 1200   reward: -2.8271895002199705\n",
            "Episode: 1201   reward: -7.293107418687641\n",
            "Episode: 1202   reward: -2.1773328295667813\n",
            "Episode: 1203   reward: -2.995135207034248\n",
            "Episode: 1204   reward: -3.0866586499849196\n",
            "Episode: 1205   reward: -6.282312610596056\n",
            "Episode: 1206   reward: -5.037444987387534\n",
            "Episode: 1207   reward: -7.486612138479236\n",
            "Episode: 1208   reward: -1.6389278625031292\n",
            "Episode: 1209   reward: -10.238056637646313\n",
            "Episode: 1210   reward: -19.023823313790732\n",
            "Episode: 1211   reward: -9.998824342790096\n",
            "Episode: 1212   reward: -13.192097320400675\n",
            "Episode: 1213   reward: -9.8836072813617\n",
            "Episode: 1214   reward: -9.99686948259747\n",
            "Episode: 1215   reward: -4.57776634479342\n",
            "Episode: 1216   reward: -1.3080030706455756\n",
            "Episode: 1217   reward: -2.49260444488778\n",
            "Episode: 1218   reward: -1.9445985821030904\n",
            "Episode: 1219   reward: -8.05795644441469\n",
            "Episode: 1220   reward: -0.6355814583268722\n",
            "Episode: 1221   reward: -15.79986863100129\n",
            "Episode: 1222   reward: -7.033550880602437\n",
            "Episode: 1223   reward: -4.559340133084865\n",
            "Episode: 1224   reward: -5.6317179937043\n",
            "Episode: 1225   reward: -3.1158129169315933\n",
            "Episode: 1226   reward: -6.259392321044753\n",
            "Episode: 1227   reward: -1.785770468785178\n",
            "Episode: 1228   reward: -4.734144937568037\n",
            "Episode: 1229   reward: -1.7438009586885215\n",
            "Episode: 1230   reward: -0.7701441494835212\n",
            "Episode: 1231   reward: -5.970963884642203\n",
            "Episode: 1232   reward: -5.2482858571901945\n",
            "Episode: 1233   reward: -4.032275561213147\n",
            "Episode: 1234   reward: -8.026035309876642\n",
            "Episode: 1235   reward: -5.812579210845858\n",
            "Episode: 1236   reward: -4.090193442613769\n",
            "Episode: 1237   reward: -3.1668423368263663\n",
            "Episode: 1238   reward: -5.548794399548567\n",
            "Episode: 1239   reward: -3.3082737126429125\n",
            "Episode: 1240   reward: -2.1483594605509535\n",
            "Episode: 1241   reward: -17.208027269682162\n",
            "Episode: 1242   reward: -8.499742229372847\n",
            "Episode: 1243   reward: -8.34277232846076\n",
            "Episode: 1244   reward: -21.43277263217638\n",
            "Episode: 1245   reward: -8.278426437590491\n",
            "Episode: 1246   reward: -5.959844071808863\n",
            "Episode: 1247   reward: -2.046920945874277\n",
            "Episode: 1248   reward: -10.215728845509242\n",
            "Episode: 1249   reward: -4.023961870528033\n",
            "Episode: 1250   reward: -3.2951841105136603\n",
            "Episode: 1251   reward: -9.675223374727382\n",
            "Episode: 1252   reward: -3.6869308643431364\n",
            "Episode: 1253   reward: -7.05397058038762\n",
            "Episode: 1254   reward: -10.643898952084996\n",
            "Episode: 1255   reward: -4.632116699559463\n",
            "Episode: 1256   reward: -9.990384165319748\n",
            "Episode: 1257   reward: -2.959840377424614\n",
            "Episode: 1258   reward: -1.5826764698101374\n",
            "Episode: 1259   reward: -7.542889301434196\n",
            "Episode: 1260   reward: -2.495348063978484\n",
            "Episode: 1261   reward: -10.323903944682652\n",
            "Episode: 1262   reward: -6.194183417542383\n",
            "Episode: 1263   reward: -6.919591391504825\n",
            "Episode: 1264   reward: -5.437349360336769\n",
            "Episode: 1265   reward: -2.9310902911993146\n",
            "Episode: 1266   reward: -12.93753200680466\n",
            "Episode: 1267   reward: -5.211984749903098\n",
            "Episode: 1268   reward: -2.1293489676554853\n",
            "Episode: 1269   reward: -3.8498658522575995\n",
            "Episode: 1270   reward: -16.721122606627002\n",
            "Episode: 1271   reward: -2.65611553094308\n",
            "Episode: 1272   reward: -11.106881253540312\n",
            "Episode: 1273   reward: -3.2344456341485257\n",
            "Episode: 1274   reward: -1.9019590027994837\n",
            "Episode: 1275   reward: -1.010599985874087\n",
            "Episode: 1276   reward: -1.9946557977144799\n",
            "Episode: 1277   reward: -2.0467387032209237\n",
            "Episode: 1278   reward: -3.9186247942774197\n",
            "Episode: 1279   reward: -2.089369504926819\n",
            "Episode: 1280   reward: -8.644828473615387\n",
            "Episode: 1281   reward: -2.6930105228319516\n",
            "Episode: 1282   reward: -3.1503698422579665\n",
            "Episode: 1283   reward: -11.381347551878937\n",
            "Episode: 1284   reward: -4.707297110060008\n",
            "Episode: 1285   reward: -1.9276862691529273\n",
            "Episode: 1286   reward: -4.211844952326361\n",
            "Episode: 1287   reward: -6.1112069426444835\n",
            "Episode: 1288   reward: -11.589793829356235\n",
            "Episode: 1289   reward: -8.756509262543442\n",
            "Episode: 1290   reward: -12.070156871558105\n",
            "Episode: 1291   reward: -2.9643464027938697\n",
            "Episode: 1292   reward: -2.810487553134644\n",
            "Episode: 1293   reward: -1.337810611869432\n",
            "Episode: 1294   reward: -4.987039301672504\n",
            "Episode: 1295   reward: -8.256765033877421\n",
            "Episode: 1296   reward: -5.943005523293485\n",
            "Episode: 1297   reward: -1.931447364160719\n",
            "Episode: 1298   reward: -1.928299101683066\n",
            "Episode: 1299   reward: -2.373348036012996\n",
            "Episode: 1300   reward: -5.882650611852163\n",
            "Episode: 1301   reward: -12.680840738382791\n",
            "Episode: 1302   reward: -12.516645318143548\n",
            "Episode: 1303   reward: -3.298309859257794\n",
            "Episode: 1304   reward: -3.8150508159620875\n",
            "Episode: 1305   reward: -2.0744200914240065\n",
            "Episode: 1306   reward: -7.8748307609423085\n",
            "Episode: 1307   reward: -7.176086649181161\n",
            "Episode: 1308   reward: -18.254408754893525\n",
            "Episode: 1309   reward: -4.638115888897044\n",
            "Episode: 1310   reward: -2.0671670417901176\n",
            "Episode: 1311   reward: -2.4440558469257923\n",
            "Episode: 1312   reward: -0.8997639479899499\n",
            "Episode: 1313   reward: -1.8947811189296944\n",
            "Episode: 1314   reward: -6.963513807614563\n",
            "Episode: 1315   reward: -4.1187691725406\n",
            "Episode: 1316   reward: -4.179247264610319\n",
            "Episode: 1317   reward: -1.3189409167889017\n",
            "Episode: 1318   reward: -1.136009423323267\n",
            "Episode: 1319   reward: -1.2015603817792588\n",
            "Episode: 1320   reward: -3.3804894862092754\n",
            "Episode: 1321   reward: -3.2415495254154747\n",
            "Episode: 1322   reward: -1.463107655826288\n",
            "Episode: 1323   reward: -4.369888193052946\n",
            "Episode: 1324   reward: -0.8747875104980184\n",
            "Episode: 1325   reward: -4.222802233148826\n",
            "Episode: 1326   reward: -11.013508179302189\n",
            "Episode: 1327   reward: -3.3963225804415207\n",
            "Episode: 1328   reward: -3.4539374845317377\n",
            "Episode: 1329   reward: -1.2971132177853824\n",
            "Episode: 1330   reward: -3.073736025007732\n",
            "Episode: 1331   reward: -8.485172133323747\n",
            "Episode: 1332   reward: -10.074956042399272\n",
            "Episode: 1333   reward: -3.2855745618775924\n",
            "Episode: 1334   reward: -4.965784808081427\n",
            "Episode: 1335   reward: -1.5549343512371718\n",
            "Episode: 1336   reward: -4.199696230891975\n",
            "Episode: 1337   reward: -3.3278240032914264\n",
            "Episode: 1338   reward: -1.3031594660887722\n",
            "Episode: 1339   reward: -6.470796902675448\n",
            "Episode: 1340   reward: -13.520407477810286\n",
            "Episode: 1341   reward: -2.1654166788270857\n",
            "Episode: 1342   reward: -4.094489992148148\n",
            "Episode: 1343   reward: -1.9391087256728237\n",
            "Episode: 1344   reward: -3.2292637523671273\n",
            "Episode: 1345   reward: -1.828455101868813\n",
            "Episode: 1346   reward: -6.322290664233077\n",
            "Episode: 1347   reward: -1.1377956201395107\n",
            "Episode: 1348   reward: -6.28765009622537\n",
            "Episode: 1349   reward: -1.704831992507801\n",
            "Episode: 1350   reward: -3.4140445603136618\n",
            "Episode: 1351   reward: -6.972464145052939\n",
            "Episode: 1352   reward: -10.940524322299293\n",
            "Episode: 1353   reward: -2.183706902551291\n",
            "Episode: 1354   reward: -2.7534851578574404\n",
            "Episode: 1355   reward: -3.820004354664899\n",
            "Episode: 1356   reward: -10.166220900781246\n",
            "Episode: 1357   reward: -2.3005540185788567\n",
            "Episode: 1358   reward: -4.365717477127442\n",
            "Episode: 1359   reward: -2.293106116143195\n",
            "Episode: 1360   reward: -5.024941909943255\n",
            "Episode: 1361   reward: -4.939356135133036\n",
            "Episode: 1362   reward: -9.73863569440693\n",
            "Episode: 1363   reward: -2.8334827799969218\n",
            "Episode: 1364   reward: -2.7878187169313438\n",
            "Episode: 1365   reward: -2.562624947590078\n",
            "Episode: 1366   reward: -5.246270144127943\n",
            "Episode: 1367   reward: -2.187323230219846\n",
            "Episode: 1368   reward: -2.5578619416361406\n",
            "Episode: 1369   reward: -4.527891869082357\n",
            "Episode: 1370   reward: -19.69526202644335\n",
            "Episode: 1371   reward: -3.3675966892572657\n",
            "Episode: 1372   reward: -1.876486935568951\n",
            "Episode: 1373   reward: -2.7494042056104475\n",
            "Episode: 1374   reward: -1.473082628199721\n",
            "Episode: 1375   reward: -2.110310723783865\n",
            "Episode: 1376   reward: -1.6476037928913292\n",
            "Episode: 1377   reward: -2.04304106119674\n",
            "Episode: 1378   reward: -3.4917123192617736\n",
            "Episode: 1379   reward: -5.380350530782413\n",
            "Episode: 1380   reward: -3.784265390720271\n",
            "Episode: 1381   reward: -33.74357092601315\n",
            "Episode: 1382   reward: -3.3807967424834704\n",
            "Episode: 1383   reward: -1.4336096779351115\n",
            "Episode: 1384   reward: -4.370368531395503\n",
            "Episode: 1385   reward: -5.671425080812708\n",
            "Episode: 1386   reward: -39.86557611996501\n",
            "Episode: 1387   reward: -11.380225828331897\n",
            "Episode: 1388   reward: -1.8247155161262327\n",
            "Episode: 1389   reward: -5.676081029424298\n",
            "Episode: 1390   reward: -11.192038820236105\n",
            "Episode: 1391   reward: -3.191423535237746\n",
            "Episode: 1392   reward: -7.737186698721194\n",
            "Episode: 1393   reward: -2.878682687399052\n",
            "Episode: 1394   reward: -8.886407976083783\n",
            "Episode: 1395   reward: -3.65343226994631\n",
            "Episode: 1396   reward: -8.701894751438148\n",
            "Episode: 1397   reward: -1.7439045298495393\n",
            "Episode: 1398   reward: -9.302697172531666\n",
            "Episode: 1399   reward: -1.598449741997043\n",
            "Episode: 1400   reward: -1.384676541255294\n",
            "Episode: 1401   reward: -5.2285466423458145\n",
            "Episode: 1402   reward: -8.9959257292478\n",
            "Episode: 1403   reward: -12.308488280066971\n",
            "Episode: 1404   reward: -2.176522663737635\n",
            "Episode: 1405   reward: -2.375811965628975\n",
            "Episode: 1406   reward: -3.782911736613525\n",
            "Episode: 1407   reward: -3.5570375805920817\n",
            "Episode: 1408   reward: -1.1894933597635928\n",
            "Episode: 1409   reward: -4.819748090802717\n",
            "Episode: 1410   reward: -13.250849056813259\n",
            "Episode: 1411   reward: -2.573873433084233\n",
            "Episode: 1412   reward: -5.4442075223923805\n",
            "Episode: 1413   reward: -13.579479151806884\n",
            "Episode: 1414   reward: -7.744207255908315\n",
            "Episode: 1415   reward: -1.4819155935474548\n",
            "Episode: 1416   reward: -10.874060682636726\n",
            "Episode: 1417   reward: -11.037846627432875\n",
            "Episode: 1418   reward: -2.54041284535216\n",
            "Episode: 1419   reward: -16.279993562247785\n",
            "Episode: 1420   reward: -11.41834920893788\n",
            "Episode: 1421   reward: -5.298762473053297\n",
            "Episode: 1422   reward: -1.8946969376275593\n",
            "Episode: 1423   reward: -10.692035556629522\n",
            "Episode: 1424   reward: -13.964242420672054\n",
            "Episode: 1425   reward: -3.937364425229451\n",
            "Episode: 1426   reward: -3.256531149450042\n",
            "Episode: 1427   reward: -3.3527560507793663\n",
            "Episode: 1428   reward: -2.3135538005460656\n",
            "Episode: 1429   reward: -1.1432688851990154\n",
            "Episode: 1430   reward: -5.406413143290787\n",
            "Episode: 1431   reward: -7.892350969701683\n",
            "Episode: 1432   reward: -2.9565544370579597\n",
            "Episode: 1433   reward: -2.8571006503638836\n",
            "Episode: 1434   reward: -3.5111575844747986\n",
            "Episode: 1435   reward: -2.2045504089039896\n",
            "Episode: 1436   reward: -5.276819277072173\n",
            "Episode: 1437   reward: -2.808365009903905\n",
            "Episode: 1438   reward: -2.6358994172423236\n",
            "Episode: 1439   reward: -2.3026672616838573\n",
            "Episode: 1440   reward: -1.078547299951661\n",
            "Episode: 1441   reward: -2.9284324300878417\n",
            "Episode: 1442   reward: -2.2396952082502666\n",
            "Episode: 1443   reward: -2.371062910348151\n",
            "Episode: 1444   reward: -5.67089957529105\n",
            "Episode: 1445   reward: -4.864802182644897\n",
            "Episode: 1446   reward: -82.42470435088221\n",
            "Episode: 1447   reward: -8.907072629452603\n",
            "Episode: 1448   reward: -0.9501855456061872\n",
            "Episode: 1449   reward: -4.723179506010176\n",
            "Episode: 1450   reward: -2.577663871636004\n",
            "Episode: 1451   reward: -17.44806689362114\n",
            "Episode: 1452   reward: -14.267568006145774\n",
            "Episode: 1453   reward: -5.384812179409183\n",
            "Episode: 1454   reward: -2.397875618940271\n",
            "Episode: 1455   reward: -1.9605244815484177\n",
            "Episode: 1456   reward: -2.6612810900610215\n",
            "Episode: 1457   reward: -2.061456818545268\n",
            "Episode: 1458   reward: -4.454487098111564\n",
            "Episode: 1459   reward: -8.48742496374361\n",
            "Episode: 1460   reward: -1.6348476260713871\n",
            "Episode: 1461   reward: -0.77926011944017\n",
            "Episode: 1462   reward: -2.164723324774229\n",
            "Episode: 1463   reward: -5.920087014657267\n",
            "Episode: 1464   reward: -14.387853397971858\n",
            "Episode: 1465   reward: -1.7518878504746722\n",
            "Episode: 1466   reward: -2.0112318966544556\n",
            "Episode: 1467   reward: -16.04867309586032\n",
            "Episode: 1468   reward: -1.896044847832792\n",
            "Episode: 1469   reward: -2.29197003259445\n",
            "Episode: 1470   reward: -1.64063694223793\n",
            "Episode: 1471   reward: -6.332313603743794\n",
            "Episode: 1472   reward: -0.8177190221545478\n",
            "Episode: 1473   reward: -3.2640902699285697\n",
            "Episode: 1474   reward: -1.2083553536743037\n",
            "Episode: 1475   reward: -13.260739874213291\n",
            "Episode: 1476   reward: -3.7967039721324607\n",
            "Episode: 1477   reward: -2.19363068928489\n",
            "Episode: 1478   reward: -2.0281988385922833\n",
            "Episode: 1479   reward: -10.286115991729359\n",
            "Episode: 1480   reward: -4.7256727731764325\n",
            "Episode: 1481   reward: -4.7659815822518405\n",
            "Episode: 1482   reward: -7.14656650940067\n",
            "Episode: 1483   reward: -6.691317419676222\n",
            "Episode: 1484   reward: -6.34038982237138\n",
            "Episode: 1485   reward: -3.67028785072382\n",
            "Episode: 1486   reward: -4.280908652067909\n",
            "Episode: 1487   reward: -4.525931501941249\n",
            "Episode: 1488   reward: -3.6963195739298067\n",
            "Episode: 1489   reward: -11.386089257789422\n",
            "Episode: 1490   reward: -3.867471077020189\n",
            "Episode: 1491   reward: -3.5319859997991614\n",
            "Episode: 1492   reward: -2.426970557708442\n",
            "Episode: 1493   reward: -3.8163939788859675\n",
            "Episode: 1494   reward: -3.92337629130414\n",
            "Episode: 1495   reward: -3.160945179637989\n",
            "Episode: 1496   reward: -6.004442086830601\n",
            "Episode: 1497   reward: -3.337077196497323\n",
            "Episode: 1498   reward: -4.492429686131056\n",
            "Episode: 1499   reward: -3.472761131202197\n"
          ]
        }
      ],
      "source": [
        "from robot import Robot\n",
        "from arm_dynamics import ArmDynamics\n",
        "\n",
        "# DO NOT CHANGE\n",
        "# ---------------\n",
        "arm = Robot(\n",
        "        ArmDynamics(\n",
        "            num_links=2,\n",
        "            link_mass=0.1,\n",
        "            link_length=1,\n",
        "            joint_viscous_friction=0.1,\n",
        "            dt=0.01,\n",
        "\t    \t\t\tgravity=False\n",
        "        )\n",
        "    )\n",
        "arm.reset()\n",
        "env = ArmEnv(arm, gui=False)\n",
        "tqdn = TrainDQN(env)\n",
        "# ---------------\n",
        "\n",
        "# Call your trin function here\n",
        "tqdn.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To keep track of your experiments, it is good practice to plot and check how well is your model trained based on the returns vs episodes plot. With a large number of episodes, this  plot may look very jagged making it difficult to ascertain how well you are doing. We are proving code to smoothen out the plot by. This will take a large list of returns in every episode and plot a smoothened version of the list. Feel free to use it if it helps.\n",
        "```\n",
        "import seaborn as sns\n",
        "returns = __\n",
        "smoothing = 10\n",
        "\n",
        "smoothened = [sum(returns[i:i+smoothing])/smoothing for i in range(0, len(returns), smoothing)]\n",
        "sns.lineplot(smoothened)\n",
        "```"
      ],
      "metadata": {
        "id": "jAlaOcVJsn5a"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIcBDbeTRNZI"
      },
      "source": [
        "### Load your model and test its performance\n",
        "Change your model path and the goal to see how well your learnt model is performing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5gTRNKhRQQM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92e2d100-bd87-4690-b8c8-9215a498eb1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode return:  -2.880627682702006\n"
          ]
        }
      ],
      "source": [
        "import collections\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "from render import Renderer\n",
        "from arm_env import ArmEnv\n",
        "import numpy as np\n",
        "import os\n",
        "from math import dist\n",
        "import seaborn as sns\n",
        "from robot import Robot\n",
        "from arm_dynamics import ArmDynamics\n",
        "from geometry import polar2cartesian\n",
        "\n",
        "\n",
        "# DO NOT CHANGE arm parameters\n",
        "arm = Robot(\n",
        "        ArmDynamics(\n",
        "            num_links=2,\n",
        "            link_mass=0.1,\n",
        "            link_length=1,\n",
        "            joint_viscous_friction=0.1,\n",
        "            dt=0.01,\n",
        "\t    \t\t\tgravity=False\n",
        "        )\n",
        "    )\n",
        "arm.reset()\n",
        "# ------------------\n",
        "\n",
        "env = ArmEnv(arm, gui=False)\n",
        "model_path = '/content/models/2023-05-03_03-38-43/q_network_ep_1000.pth' # Fill in the model_path\n",
        "device = torch.device('cpu')\n",
        "qnet = QNetwork(env).to(device)\n",
        "qnet.load_state_dict(torch.load(model_path))\n",
        "qnet.eval()\n",
        "goal = polar2cartesian(1.6, 0.25 - np.pi/2.0)\n",
        "done = False\n",
        "obs = env.reset(goal)\n",
        "\n",
        "episode_return = 0\n",
        "while not done:\n",
        "  action = qnet.select_discrete_action(obs, device)\n",
        "  action = qnet.action_discrete_to_continuous(action)\n",
        "  new_obs, reward, done, info = env.step(action)\n",
        "  episode_return += reward\n",
        "\n",
        "  pos_ee = info['pos_ee']\n",
        "  vel_ee = info['vel_ee']\n",
        "  dist = np.linalg.norm(pos_ee - goal)\n",
        "\n",
        "  obs = new_obs\n",
        "print('Episode return: ', episode_return)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Grading and Evaluation\n",
        "You will be evaluated on 5 different goal positions worth 1.5 points each. You must pass the best `model_path` for your network. The scoring function will run one episode for every goal position and find the total reward (aka return) for the episode. For every goal you get:\n",
        "\n",
        "* 1 Point if `easy target < total reward < hard target`\n",
        "* 1.5 Points if `hard target < total reward`"
      ],
      "metadata": {
        "id": "pUDEYLsZLSp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from score import compute_score\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from render import Renderer\n",
        "from arm_env import ArmEnv\n",
        "from robot import Robot\n",
        "from arm_dynamics import ArmDynamics\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# DO NOT CHANGE arm parameters\n",
        "arm = Robot(\n",
        "        ArmDynamics(\n",
        "            num_links=2,\n",
        "            link_mass=0.1,\n",
        "            link_length=1,\n",
        "            joint_viscous_friction=0.1,\n",
        "            dt=0.01,\n",
        "\t    \t\t\tgravity=False\n",
        "        )\n",
        "    )\n",
        "arm.reset()\n",
        "# ------------------\n",
        "\n",
        "env = ArmEnv(arm, gui=False)\n",
        "model_path = '/content/models/2023-05-03_03-38-43/q_network_ep_1000.pth' # Fill in the model_path\n",
        "device = torch.device('cpu')\n",
        "qnet = QNetwork(env).to(device)\n",
        "qnet.load_state_dict(torch.load(model_path))\n",
        "qnet.eval()\n",
        "score = compute_score(qnet, env, device)"
      ],
      "metadata": {
        "id": "OhPD-u6TIxdl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b279166-3e39-4445-ec55-285e01482fc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---Computing score---\n",
            "\n",
            "Goal 1:\n",
            "Total reward: -1.1449160079491505\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 1.5\n",
            "\n",
            "Goal 2:\n",
            "Total reward: -2.880627682702006\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 1.5\n",
            "\n",
            "Goal 3:\n",
            "Total reward: -2.697770644564659\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 1.5\n",
            "\n",
            "Goal 4:\n",
            "Total reward: -3.8222012495251096\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 1.5\n",
            "\n",
            "Goal 5:\n",
            "Total reward: -5.35765058493385\n",
            "easy target: -10\n",
            "hard target: -7\n",
            "points: 1.5\n",
            "\n",
            "\n",
            "Final score: 7.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: PPO with an open source RL library\n",
        "\n",
        "In this part, you will use one of the most popular open source RL libraries ([Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/)) to solve the same goal reaching problem as Part 1. We will use the same `ArmEnv` gym environment. The algorithm you should choose to use is PPO."
      ],
      "metadata": {
        "id": "vkCvO0-05XK5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PPO training\n",
        "\n",
        "We provide the code to construct parallel environments. Parallel environments can be very useful if you have good CPUs and it can speed up training."
      ],
      "metadata": {
        "id": "UBK89P2B8CgL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT CHANGE\n",
        "\n",
        "from stable_baselines3.common.vec_env.subproc_vec_env import SubprocVecEnv\n",
        "from stable_baselines3.common.vec_env.vec_monitor import VecMonitor\n",
        "from copy import deepcopy\n",
        "from robot import Robot\n",
        "from arm_dynamics import ArmDynamics\n",
        "from arm_env import ArmEnv\n",
        "\n",
        "class EnvMaker:\n",
        "    def __init__(self,  arm, seed):\n",
        "        self.seed = seed\n",
        "        self.arm = arm\n",
        "\n",
        "    def __call__(self):\n",
        "        arm = deepcopy(self.arm)\n",
        "        env = ArmEnv(arm)\n",
        "        env.seed(self.seed)\n",
        "        return env\n",
        "\n",
        "def make_vec_env(arm, nenv, seed):\n",
        "    return VecMonitor(SubprocVecEnv([EnvMaker(arm, seed  + 100 * i) for i in range(nenv)]))\n",
        "\n",
        "# conveniet function to create a robot arm\n",
        "def make_arm():\n",
        "    arm = Robot(\n",
        "        ArmDynamics(\n",
        "            num_links=2,\n",
        "            link_mass=0.1,\n",
        "            link_length=1,\n",
        "            joint_viscous_friction=0.1,\n",
        "            dt=0.01\n",
        "        )\n",
        "    )\n",
        "    arm.reset()\n",
        "    return arm\n"
      ],
      "metadata": {
        "id": "2RTqfmpVwMja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will need to complete the code to train the policy using the [PPO class](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html) from stable_baselines3. We provide the code to generate the name of the directory to save the checkpoint, an example is `ppo_models/2023-04-13_01-14-13`. Your checkpoint model should be named `ppo_network.zip`. See the [save](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#stable_baselines3.ppo.PPO.save) function. Training should take less than 40 minutes."
      ],
      "metadata": {
        "id": "Bniz2TouwM3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3.ppo import PPO\n",
        "import os\n",
        "import time\n",
        "from stable_baselines3.common.utils import set_random_seed\n",
        "\n",
        "\n",
        "# Default parameters\n",
        "timesteps = 500000\n",
        "nenv = 8  # number of parallel environments. This can speed up training when you have good CPUs\n",
        "seed = 8\n",
        "batch_size = 2048\n",
        "\n",
        "# Generate path of the directory to save the checkpoint\n",
        "timestr = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "save_dir = os.path.join('ppo_models', timestr)\n",
        "\n",
        "# Set random seed\n",
        "set_random_seed(seed)\n",
        "\n",
        "# Create arm\n",
        "arm = make_arm()\n",
        "\n",
        "# Create parallel envs\n",
        "vec_env = make_vec_env(arm=arm, nenv=nenv, seed=seed)\n",
        "\n",
        "# ------ IMPLEMENT YOUR TRAINING CODE HERE ------------\n",
        "# raise NotImplementedError\n",
        "\n",
        "\n",
        "# Create PPO model\n",
        "ppo_model = PPO(\"MlpPolicy\", vec_env, verbose=1, n_steps=batch_size, seed=seed, tensorboard_log=\"./ppo_tensorboard/\")\n",
        "\n",
        "# Train PPO model\n",
        "ppo_model.learn(total_timesteps=timesteps)\n",
        "\n",
        "# Save the trained model\n",
        "if not os.path.exists(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "\n",
        "model_save_path = os.path.join(save_dir, \"ppo_network.zip\")\n",
        "ppo_model.save(model_save_path)\n",
        "print(f\"Model saved to {model_save_path}\")\n",
        "# Do not forget to save your model at the end of training"
      ],
      "metadata": {
        "id": "FHoSWOnG-2sH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08dc1718-1093-496a-dce8-a806795dfeee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "Logging to ./ppo_tensorboard/PPO_1\n",
            "-----------------------------------\n",
            "| rollout/           |            |\n",
            "|    ep_len_mean     | 200        |\n",
            "|    ep_rew_mean     | -200.53925 |\n",
            "| time/              |            |\n",
            "|    fps             | 696        |\n",
            "|    iterations      | 1          |\n",
            "|    time_elapsed    | 23         |\n",
            "|    total_timesteps | 16384      |\n",
            "-----------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -140.84149   |\n",
            "| time/                   |              |\n",
            "|    fps                  | 579          |\n",
            "|    iterations           | 2            |\n",
            "|    time_elapsed         | 56           |\n",
            "|    total_timesteps      | 32768        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0060855187 |\n",
            "|    clip_fraction        | 0.0604       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.85        |\n",
            "|    explained_variance   | 0.00222      |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 31.5         |\n",
            "|    n_updates            | 10           |\n",
            "|    policy_gradient_loss | -0.00542     |\n",
            "|    std                  | 1.01         |\n",
            "|    value_loss           | 242          |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -89.754906   |\n",
            "| time/                   |              |\n",
            "|    fps                  | 556          |\n",
            "|    iterations           | 3            |\n",
            "|    time_elapsed         | 88           |\n",
            "|    total_timesteps      | 49152        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0069103083 |\n",
            "|    clip_fraction        | 0.08         |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.85        |\n",
            "|    explained_variance   | 0.444        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 50.6         |\n",
            "|    n_updates            | 20           |\n",
            "|    policy_gradient_loss | -0.00801     |\n",
            "|    std                  | 1.01         |\n",
            "|    value_loss           | 94.2         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -45.77525   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 546         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 119         |\n",
            "|    total_timesteps      | 65536       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008549693 |\n",
            "|    clip_fraction        | 0.105       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.83       |\n",
            "|    explained_variance   | 0.516       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 16.7        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.0108     |\n",
            "|    std                  | 0.995       |\n",
            "|    value_loss           | 42.7        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -31.847864  |\n",
            "| time/                   |             |\n",
            "|    fps                  | 537         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 152         |\n",
            "|    total_timesteps      | 81920       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008801997 |\n",
            "|    clip_fraction        | 0.113       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.8        |\n",
            "|    explained_variance   | 0.481       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 8.18        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.0106     |\n",
            "|    std                  | 0.979       |\n",
            "|    value_loss           | 15.4        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -24.8396     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 531          |\n",
            "|    iterations           | 6            |\n",
            "|    time_elapsed         | 184          |\n",
            "|    total_timesteps      | 98304        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0076784734 |\n",
            "|    clip_fraction        | 0.103        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.77        |\n",
            "|    explained_variance   | 0.398        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.5          |\n",
            "|    n_updates            | 50           |\n",
            "|    policy_gradient_loss | -0.00896     |\n",
            "|    std                  | 0.966        |\n",
            "|    value_loss           | 7.36         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -22.477007  |\n",
            "| time/                   |             |\n",
            "|    fps                  | 529         |\n",
            "|    iterations           | 7           |\n",
            "|    time_elapsed         | 216         |\n",
            "|    total_timesteps      | 114688      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006741765 |\n",
            "|    clip_fraction        | 0.0775      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.74       |\n",
            "|    explained_variance   | 0.348       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.34        |\n",
            "|    n_updates            | 60          |\n",
            "|    policy_gradient_loss | -0.00384    |\n",
            "|    std                  | 0.947       |\n",
            "|    value_loss           | 4.9         |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -19.28473    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 528          |\n",
            "|    iterations           | 8            |\n",
            "|    time_elapsed         | 248          |\n",
            "|    total_timesteps      | 131072       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0064207986 |\n",
            "|    clip_fraction        | 0.074        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.71        |\n",
            "|    explained_variance   | 0.452        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.58         |\n",
            "|    n_updates            | 70           |\n",
            "|    policy_gradient_loss | -0.00248     |\n",
            "|    std                  | 0.939        |\n",
            "|    value_loss           | 3.68         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -16.117456   |\n",
            "| time/                   |              |\n",
            "|    fps                  | 524          |\n",
            "|    iterations           | 9            |\n",
            "|    time_elapsed         | 281          |\n",
            "|    total_timesteps      | 147456       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0064843106 |\n",
            "|    clip_fraction        | 0.0732       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.7         |\n",
            "|    explained_variance   | 0.589        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.57         |\n",
            "|    n_updates            | 80           |\n",
            "|    policy_gradient_loss | -0.00347     |\n",
            "|    std                  | 0.933        |\n",
            "|    value_loss           | 2.98         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -14.899771   |\n",
            "| time/                   |              |\n",
            "|    fps                  | 523          |\n",
            "|    iterations           | 10           |\n",
            "|    time_elapsed         | 312          |\n",
            "|    total_timesteps      | 163840       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0065595917 |\n",
            "|    clip_fraction        | 0.073        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.69        |\n",
            "|    explained_variance   | 0.636        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.66         |\n",
            "|    n_updates            | 90           |\n",
            "|    policy_gradient_loss | -0.00178     |\n",
            "|    std                  | 0.932        |\n",
            "|    value_loss           | 2.5          |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -14.010608   |\n",
            "| time/                   |              |\n",
            "|    fps                  | 523          |\n",
            "|    iterations           | 11           |\n",
            "|    time_elapsed         | 344          |\n",
            "|    total_timesteps      | 180224       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0073391586 |\n",
            "|    clip_fraction        | 0.072        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.67        |\n",
            "|    explained_variance   | 0.748        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.32         |\n",
            "|    n_updates            | 100          |\n",
            "|    policy_gradient_loss | -0.00197     |\n",
            "|    std                  | 0.916        |\n",
            "|    value_loss           | 1.71         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -14.185586  |\n",
            "| time/                   |             |\n",
            "|    fps                  | 523         |\n",
            "|    iterations           | 12          |\n",
            "|    time_elapsed         | 375         |\n",
            "|    total_timesteps      | 196608      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007520535 |\n",
            "|    clip_fraction        | 0.0909      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.64       |\n",
            "|    explained_variance   | 0.796       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.72        |\n",
            "|    n_updates            | 110         |\n",
            "|    policy_gradient_loss | -0.00394    |\n",
            "|    std                  | 0.901       |\n",
            "|    value_loss           | 1.64        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 200        |\n",
            "|    ep_rew_mean          | -12.894517 |\n",
            "| time/                   |            |\n",
            "|    fps                  | 521        |\n",
            "|    iterations           | 13         |\n",
            "|    time_elapsed         | 408        |\n",
            "|    total_timesteps      | 212992     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00839888 |\n",
            "|    clip_fraction        | 0.102      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -2.61      |\n",
            "|    explained_variance   | 0.78       |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 0.636      |\n",
            "|    n_updates            | 120        |\n",
            "|    policy_gradient_loss | -0.00588   |\n",
            "|    std                  | 0.889      |\n",
            "|    value_loss           | 1.36       |\n",
            "----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -10.453338   |\n",
            "| time/                   |              |\n",
            "|    fps                  | 520          |\n",
            "|    iterations           | 14           |\n",
            "|    time_elapsed         | 440          |\n",
            "|    total_timesteps      | 229376       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0084632775 |\n",
            "|    clip_fraction        | 0.0974       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.58        |\n",
            "|    explained_variance   | 0.768        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.638        |\n",
            "|    n_updates            | 130          |\n",
            "|    policy_gradient_loss | -0.00648     |\n",
            "|    std                  | 0.875        |\n",
            "|    value_loss           | 1.59         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -8.037116   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 520         |\n",
            "|    iterations           | 15          |\n",
            "|    time_elapsed         | 471         |\n",
            "|    total_timesteps      | 245760      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009748412 |\n",
            "|    clip_fraction        | 0.12        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.52       |\n",
            "|    explained_variance   | 0.822       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.187       |\n",
            "|    n_updates            | 140         |\n",
            "|    policy_gradient_loss | -0.00596    |\n",
            "|    std                  | 0.855       |\n",
            "|    value_loss           | 0.586       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -6.6329827  |\n",
            "| time/                   |             |\n",
            "|    fps                  | 520         |\n",
            "|    iterations           | 16          |\n",
            "|    time_elapsed         | 503         |\n",
            "|    total_timesteps      | 262144      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009424352 |\n",
            "|    clip_fraction        | 0.114       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.5        |\n",
            "|    explained_variance   | 0.847       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.118       |\n",
            "|    n_updates            | 150         |\n",
            "|    policy_gradient_loss | -0.00628    |\n",
            "|    std                  | 0.842       |\n",
            "|    value_loss           | 0.313       |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 200        |\n",
            "|    ep_rew_mean          | -5.9764595 |\n",
            "| time/                   |            |\n",
            "|    fps                  | 518        |\n",
            "|    iterations           | 17         |\n",
            "|    time_elapsed         | 537        |\n",
            "|    total_timesteps      | 278528     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00905014 |\n",
            "|    clip_fraction        | 0.103      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -2.47      |\n",
            "|    explained_variance   | 0.846      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 0.0725     |\n",
            "|    n_updates            | 160        |\n",
            "|    policy_gradient_loss | -0.00492   |\n",
            "|    std                  | 0.835      |\n",
            "|    value_loss           | 0.244      |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -5.3366485  |\n",
            "| time/                   |             |\n",
            "|    fps                  | 518         |\n",
            "|    iterations           | 18          |\n",
            "|    time_elapsed         | 568         |\n",
            "|    total_timesteps      | 294912      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010540191 |\n",
            "|    clip_fraction        | 0.126       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.43       |\n",
            "|    explained_variance   | 0.867       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0979      |\n",
            "|    n_updates            | 170         |\n",
            "|    policy_gradient_loss | -0.0071     |\n",
            "|    std                  | 0.815       |\n",
            "|    value_loss           | 0.129       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -4.3607783  |\n",
            "| time/                   |             |\n",
            "|    fps                  | 518         |\n",
            "|    iterations           | 19          |\n",
            "|    time_elapsed         | 600         |\n",
            "|    total_timesteps      | 311296      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011563506 |\n",
            "|    clip_fraction        | 0.147       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.38       |\n",
            "|    explained_variance   | 0.896       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0296      |\n",
            "|    n_updates            | 180         |\n",
            "|    policy_gradient_loss | -0.00883    |\n",
            "|    std                  | 0.797       |\n",
            "|    value_loss           | 0.0819      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -3.8999066  |\n",
            "| time/                   |             |\n",
            "|    fps                  | 516         |\n",
            "|    iterations           | 20          |\n",
            "|    time_elapsed         | 634         |\n",
            "|    total_timesteps      | 327680      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010548554 |\n",
            "|    clip_fraction        | 0.12        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.33       |\n",
            "|    explained_variance   | 0.883       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0359      |\n",
            "|    n_updates            | 190         |\n",
            "|    policy_gradient_loss | -0.003      |\n",
            "|    std                  | 0.776       |\n",
            "|    value_loss           | 0.0451      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -3.8784068  |\n",
            "| time/                   |             |\n",
            "|    fps                  | 516         |\n",
            "|    iterations           | 21          |\n",
            "|    time_elapsed         | 666         |\n",
            "|    total_timesteps      | 344064      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010465087 |\n",
            "|    clip_fraction        | 0.118       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.27       |\n",
            "|    explained_variance   | 0.907       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.00422     |\n",
            "|    n_updates            | 200         |\n",
            "|    policy_gradient_loss | -0.00358    |\n",
            "|    std                  | 0.753       |\n",
            "|    value_loss           | 0.0353      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -3.4933536  |\n",
            "| time/                   |             |\n",
            "|    fps                  | 516         |\n",
            "|    iterations           | 22          |\n",
            "|    time_elapsed         | 698         |\n",
            "|    total_timesteps      | 360448      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009691468 |\n",
            "|    clip_fraction        | 0.115       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.21       |\n",
            "|    explained_variance   | 0.912       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0019      |\n",
            "|    n_updates            | 210         |\n",
            "|    policy_gradient_loss | -0.00308    |\n",
            "|    std                  | 0.73        |\n",
            "|    value_loss           | 0.0256      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -3.038329   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 516         |\n",
            "|    iterations           | 23          |\n",
            "|    time_elapsed         | 729         |\n",
            "|    total_timesteps      | 376832      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012613339 |\n",
            "|    clip_fraction        | 0.133       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.14       |\n",
            "|    explained_variance   | 0.929       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.012       |\n",
            "|    n_updates            | 220         |\n",
            "|    policy_gradient_loss | -0.00267    |\n",
            "|    std                  | 0.705       |\n",
            "|    value_loss           | 0.0177      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -3.133666   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 516         |\n",
            "|    iterations           | 24          |\n",
            "|    time_elapsed         | 761         |\n",
            "|    total_timesteps      | 393216      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011581661 |\n",
            "|    clip_fraction        | 0.134       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.07       |\n",
            "|    explained_variance   | 0.959       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0235     |\n",
            "|    n_updates            | 230         |\n",
            "|    policy_gradient_loss | -0.00253    |\n",
            "|    std                  | 0.679       |\n",
            "|    value_loss           | 0.00988     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -3.2157986  |\n",
            "| time/                   |             |\n",
            "|    fps                  | 515         |\n",
            "|    iterations           | 25          |\n",
            "|    time_elapsed         | 794         |\n",
            "|    total_timesteps      | 409600      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012866721 |\n",
            "|    clip_fraction        | 0.15        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2          |\n",
            "|    explained_variance   | 0.964       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.00602     |\n",
            "|    n_updates            | 240         |\n",
            "|    policy_gradient_loss | -0.00346    |\n",
            "|    std                  | 0.654       |\n",
            "|    value_loss           | 0.00844     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -2.8539004  |\n",
            "| time/                   |             |\n",
            "|    fps                  | 515         |\n",
            "|    iterations           | 26          |\n",
            "|    time_elapsed         | 825         |\n",
            "|    total_timesteps      | 425984      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013298731 |\n",
            "|    clip_fraction        | 0.154       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.93       |\n",
            "|    explained_variance   | 0.97        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0138      |\n",
            "|    n_updates            | 250         |\n",
            "|    policy_gradient_loss | -0.00161    |\n",
            "|    std                  | 0.634       |\n",
            "|    value_loss           | 0.00611     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -3.0847747  |\n",
            "| time/                   |             |\n",
            "|    fps                  | 515         |\n",
            "|    iterations           | 27          |\n",
            "|    time_elapsed         | 857         |\n",
            "|    total_timesteps      | 442368      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013670282 |\n",
            "|    clip_fraction        | 0.162       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.87       |\n",
            "|    explained_variance   | 0.979       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0215     |\n",
            "|    n_updates            | 260         |\n",
            "|    policy_gradient_loss | -0.00143    |\n",
            "|    std                  | 0.617       |\n",
            "|    value_loss           | 0.00542     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -2.894516   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 515         |\n",
            "|    iterations           | 28          |\n",
            "|    time_elapsed         | 889         |\n",
            "|    total_timesteps      | 458752      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010958417 |\n",
            "|    clip_fraction        | 0.144       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.82       |\n",
            "|    explained_variance   | 0.983       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.00801    |\n",
            "|    n_updates            | 270         |\n",
            "|    policy_gradient_loss | 0.000513    |\n",
            "|    std                  | 0.6         |\n",
            "|    value_loss           | 0.00338     |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 200        |\n",
            "|    ep_rew_mean          | -2.6819742 |\n",
            "| time/                   |            |\n",
            "|    fps                  | 515        |\n",
            "|    iterations           | 29         |\n",
            "|    time_elapsed         | 922        |\n",
            "|    total_timesteps      | 475136     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.0134814  |\n",
            "|    clip_fraction        | 0.153      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.76      |\n",
            "|    explained_variance   | 0.989      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.0172    |\n",
            "|    n_updates            | 280        |\n",
            "|    policy_gradient_loss | 0.00189    |\n",
            "|    std                  | 0.585      |\n",
            "|    value_loss           | 0.00254    |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -2.4431663  |\n",
            "| time/                   |             |\n",
            "|    fps                  | 515         |\n",
            "|    iterations           | 30          |\n",
            "|    time_elapsed         | 953         |\n",
            "|    total_timesteps      | 491520      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011919153 |\n",
            "|    clip_fraction        | 0.168       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.72       |\n",
            "|    explained_variance   | 0.989       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0192     |\n",
            "|    n_updates            | 290         |\n",
            "|    policy_gradient_loss | -0.000783   |\n",
            "|    std                  | 0.575       |\n",
            "|    value_loss           | 0.00241     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -2.5098772  |\n",
            "| time/                   |             |\n",
            "|    fps                  | 515         |\n",
            "|    iterations           | 31          |\n",
            "|    time_elapsed         | 985         |\n",
            "|    total_timesteps      | 507904      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017641885 |\n",
            "|    clip_fraction        | 0.18        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.67       |\n",
            "|    explained_variance   | 0.988       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0187     |\n",
            "|    n_updates            | 300         |\n",
            "|    policy_gradient_loss | 0.000858    |\n",
            "|    std                  | 0.558       |\n",
            "|    value_loss           | 0.00194     |\n",
            "-----------------------------------------\n",
            "Model saved to ppo_models/2023-05-03_04-00-07/ppo_network.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grading and evaluation\n",
        "\n",
        "The total number of points for Part 2 is 7.5. We will evaluate your trained model on 5 random goal locations. For each test, we assign points based on the distance between the end effector and the goal location at the end of the episode.\n",
        "\n",
        "- If 0 < distance < 0.05, you get 1.5 points.\n",
        "- If 0.05 <= distance < 0.1, you get 1 point.\n",
        "- If distance >= 0.1, you get 0 point.\n",
        "\n"
      ],
      "metadata": {
        "id": "f9N2falIz9rq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from score import score_policy\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.utils import set_random_seed\n",
        "from robot import Robot\n",
        "from arm_dynamics import ArmDynamics\n",
        "from render import Renderer\n",
        "import time\n",
        "\n",
        "# Set the path to your model\n",
        "model_path = '/content/ppo_models/2023-05-03_04-00-07/ppo_network.zip'\n",
        "\n",
        "set_random_seed(seed=100)\n",
        "\n",
        "# Create arm robot\n",
        "arm = make_arm()\n",
        "\n",
        "# Create environment\n",
        "env = ArmEnv(arm, gui=False)\n",
        "env.seed(100)\n",
        "\n",
        "# Load and test policy\n",
        "policy = PPO.load(model_path)\n",
        "score_policy(policy, env)"
      ],
      "metadata": {
        "id": "X6eQ2mzglwd0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82388050-01a9-46fa-c32d-c1c91f46df7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Computing score ---\n",
            "\n",
            "Goal 1: 1.5\n",
            "\n",
            "Goal 2: 1.5\n",
            "\n",
            "Goal 3: 1.5\n",
            "\n",
            "Goal 4: 1.5\n",
            "\n",
            "Goal 5: 1.5\n",
            "\n",
            "\n",
            "---\n",
            "Final score: 7.5/7.5\n",
            "---\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7.5"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    }
  ]
}
